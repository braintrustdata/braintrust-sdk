"""
Do not import this file directly. See __init__.py for the classes that have a stable API.

Auto-generated file -- do not modify.
"""

from __future__ import annotations

from typing import Any, Literal, Mapping, Sequence, TypedDict, Union

from typing_extensions import NotRequired


class Origin(TypedDict):
    object_type: Union[
        Literal["experiment", "dataset", "prompt", "function", "prompt_session"], Literal["project_logs"]
    ]
    """
    Type of the object the event is originating from.
    """
    object_id: str
    """
    ID of the object the event is originating from.
    """
    id: str
    """
    ID of the original event.
    """
    _xact_id: str
    """
    Transaction ID of the original event.
    """


class DatasetEvent(TypedDict):
    id: str
    """
    A unique identifier for the dataset event. If you don't provide one, BrainTrust will generate one for you
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the dataset (see the `version` parameter)
    """
    created: str
    """
    The timestamp the dataset event was created
    """
    project_id: str
    """
    Unique identifier for the project that the dataset belongs under
    """
    dataset_id: str
    """
    Unique identifier for the dataset
    """
    input: NotRequired[Any]
    """
    The argument that uniquely define an input case (an arbitrary, JSON serializable object)
    """
    expected: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object)
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    span_id: str
    """
    A unique identifier used to link different dataset events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this dataset event belongs to
    """
    is_root: NotRequired[bool]
    """
    Whether this span is a root span
    """
    origin: NotRequired[Origin]
    """
    Indicates the event was copied from another object.
    """
    output: NotRequired[Any]
    """
    Deprecated.
    """


class SpanAttributes(TypedDict):
    name: NotRequired[str]
    """
    Name of the span, for display purposes only
    """
    type: NotRequired[Literal["llm", "score", "function", "eval", "task", "tool"]]
    """
    Type of the span, for display purposes only
    """


class Metrics(TypedDict):
    start: NotRequired[float]
    """
    A unix timestamp recording when the section of code which produced the experiment event started
    """
    end: NotRequired[float]
    """
    A unix timestamp recording when the section of code which produced the experiment event finished
    """
    prompt_tokens: NotRequired[int]
    """
    The number of tokens in the prompt used to generate the experiment event (only set if this is an LLM span)
    """
    completion_tokens: NotRequired[int]
    """
    The number of tokens in the completion generated by the model (only set if this is an LLM span)
    """
    tokens: NotRequired[int]
    """
    The total number of tokens in the input and output of the experiment event.
    """
    caller_functionname: NotRequired[Any]
    """
    This metric is deprecated
    """
    caller_filename: NotRequired[Any]
    """
    This metric is deprecated
    """
    caller_lineno: NotRequired[Any]
    """
    This metric is deprecated
    """


class Context(TypedDict):
    caller_functionname: NotRequired[str]
    """
    The function in code which created the experiment event
    """
    caller_filename: NotRequired[str]
    """
    Name of the file in code where the experiment event was created
    """
    caller_lineno: NotRequired[int]
    """
    Line of code where the experiment event was created
    """


class ExperimentEvent(TypedDict):
    id: str
    """
    A unique identifier for the experiment event. If you don't provide one, BrainTrust will generate one for you
    """
    dataset_record_id: NotRequired[str]
    """
    If the experiment is associated to a dataset, this is the event-level dataset id this experiment event is tied to
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the experiment (see the `version` parameter)
    """
    created: str
    """
    The timestamp the experiment event was created
    """
    project_id: str
    """
    Unique identifier for the project that the experiment belongs under
    """
    experiment_id: str
    """
    Unique identifier for the experiment
    """
    input: NotRequired[Any]
    """
    The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical
    """
    output: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models
    """
    error: NotRequired[Any]
    """
    The error that occurred, if any.
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    metrics: NotRequired[Metrics]
    """
    Metrics are numerical measurements tracking the execution of the code that produced the experiment event. Use "start" and "end" to track the time span over which the experiment event was produced
    """
    context: NotRequired[Context]
    """
    Context is additional information about the code that produced the experiment event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the experiment event
    """
    span_id: str
    """
    A unique identifier used to link different experiment events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    span_parents: NotRequired[Sequence[str]]
    """
    An array of the parent `span_ids` of this experiment event. This should be empty for the root span of a trace, and should most often contain just one parent element for subspans
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this experiment event belongs to
    """
    span_attributes: NotRequired[SpanAttributes]
    is_root: NotRequired[bool]
    """
    Whether this span is a root span
    """
    origin: NotRequired[Origin]
    """
    Indicates the event was copied from another object.
    """


class ResponseFormat(TypedDict):
    type: Literal["json_object"]


class JsonSchema(TypedDict):
    name: str
    description: NotRequired[str]
    schema_: NotRequired[Mapping[str, Any]]
    strict: NotRequired[bool]


class ResponseFormat1(TypedDict):
    type: Literal["json_schema"]
    json_schema: JsonSchema


class ResponseFormat2(TypedDict):
    type: Literal["text"]


class Function(TypedDict):
    name: str


class ToolChoice(TypedDict):
    type: Literal["function"]
    function: Function


class FunctionCall(TypedDict):
    name: str


class ModelParams1(TypedDict):
    use_cache: NotRequired[bool]
    temperature: NotRequired[float]
    top_p: NotRequired[float]
    max_tokens: NotRequired[float]
    frequency_penalty: NotRequired[float]
    presence_penalty: NotRequired[float]
    response_format: NotRequired[Union[ResponseFormat, ResponseFormat1, ResponseFormat2, Mapping[str, Any]]]
    tool_choice: NotRequired[Union[Literal["auto"], Literal["none"], Literal["required"], ToolChoice]]
    function_call: NotRequired[Union[Literal["auto"], Literal["none"], FunctionCall]]
    n: NotRequired[float]
    stop: NotRequired[Sequence[str]]


class ModelParams2(TypedDict):
    use_cache: NotRequired[bool]
    max_tokens: float
    temperature: float
    top_p: NotRequired[float]
    top_k: NotRequired[float]
    stop_sequences: NotRequired[Sequence[str]]
    max_tokens_to_sample: NotRequired[float]
    """
    This is a legacy parameter that should not be used.
    """


class ModelParams3(TypedDict):
    use_cache: NotRequired[bool]
    temperature: NotRequired[float]
    maxOutputTokens: NotRequired[float]
    topP: NotRequired[float]
    topK: NotRequired[float]


class ModelParams4(TypedDict):
    use_cache: NotRequired[bool]
    temperature: NotRequired[float]
    topK: NotRequired[float]


class ModelParams5(TypedDict):
    use_cache: NotRequired[bool]


ModelParams = Union[ModelParams1, ModelParams2, ModelParams3, ModelParams4, ModelParams5]


class PromptOptions(TypedDict):
    model: NotRequired[str]
    params: NotRequired[ModelParams]
    position: NotRequired[str]


__all__ = []
