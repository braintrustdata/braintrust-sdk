"""Do not import this file directly. See __init__.py for the classes that are considered stable.

Auto-generated file -- do not modify."""

from __future__ import annotations

from typing import Any, Literal, Mapping, Optional, Sequence, TypedDict, Union

from typing_extensions import NotRequired

ProjectIdParam = str


ExperimentIdParam = str


DatasetIdParam = str


PromptIdParam = str


PromptSessionIdParam = str


RoleIdParam = str


GroupIdParam = str


AclIdParam = str


UserIdParam = str


ProjectScoreIdParam = str


ProjectTagIdParam = str


SpanIframeIdParam = str


FunctionIdParam = str


ViewIdParam = str


OrganizationIdParam = str


ApiKeyIdParam = str


AiSecretIdParam = str


EnvVarIdParam = str


ProjectIdQuery = str


ProjectName = str


ExperimentName = str


DatasetName = str


PromptName = str


PromptSessionName = str


RoleName = str


GroupName = str


ProjectScoreName = str


ProjectTagName = str


SpanIframeName = str


FunctionName = str


ViewName = str


ApiKeyName = str


AiSecretName = str


EnvVarName = str


OrgName = str


Ids = Union[str, Sequence[str]]


AppLimitParam = Optional[int]


FetchLimitParam = Optional[int]


StartingAfter = str


EndingBefore = str


MaxXactId = str


MaxRootSpanId = str


Version = str


PromptVersion = str


SummarizeScores = Optional[bool]


ComparisonExperimentId = str


SummarizeData = Optional[bool]


Slug = str


ViewType = Literal["projects", "logs", "experiments", "datasets", "prompts", "playgrounds", "experiment", "dataset"]


UserGivenName = Union[str, Sequence[str]]


UserFamilyName = Union[str, Sequence[str]]


UserEmail = Union[str, Sequence[str]]


AclObjectType = Literal[
    "organization",
    "project",
    "experiment",
    "dataset",
    "prompt",
    "prompt_session",
    "group",
    "role",
    "org_member",
    "project_log",
    "org_project",
]


AclObjectId = str


ProjectScoreType = Literal["slider", "categorical", "weighted", "minimum", "maximum", "online"]


AISecretType = Union[str, Sequence[str]]


EnvVarObjectType = Literal["organization", "project", "function"]


EnvVarObjectId = str


class ProjectSettings(TypedDict):
    comparison_key: NotRequired[str]
    """
    The key used to join two experiments (defaults to `input`).
    """


class Project(TypedDict):
    id: str
    """
    Unique identifier for the project
    """
    org_id: str
    """
    Unique id for the organization that the project belongs under
    """
    name: str
    """
    Name of the project
    """
    created: NotRequired[str]
    """
    Date of project creation
    """
    deleted_at: NotRequired[str]
    """
    Date of project deletion, or null if the project is still active
    """
    user_id: NotRequired[str]
    """
    Identifies the user who created the project
    """
    settings: NotRequired[ProjectSettings]


class CreateProject(TypedDict):
    name: str
    """
    Name of the project
    """
    org_name: NotRequired[str]
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the project belongs in.
    """


class PatchProject(TypedDict):
    name: NotRequired[str]
    """
    Name of the project
    """
    settings: NotRequired[ProjectSettings]


class InsertEventsResponse(TypedDict):
    row_ids: Sequence[str]
    """
    The ids of all rows that were inserted, aligning one-to-one with the rows provided as input
    """


class Metrics(TypedDict):
    start: NotRequired[float]
    """
    A unix timestamp recording when the section of code which produced the project logs event started
    """
    end: NotRequired[float]
    """
    A unix timestamp recording when the section of code which produced the project logs event finished
    """
    prompt_tokens: NotRequired[int]
    """
    The number of tokens in the prompt used to generate the project logs event (only set if this is an LLM span)
    """
    completion_tokens: NotRequired[int]
    """
    The number of tokens in the completion generated by the model (only set if this is an LLM span)
    """
    tokens: NotRequired[int]
    """
    The total number of tokens in the input and output of the project logs event.
    """


class Context(TypedDict):
    caller_functionname: NotRequired[str]
    """
    The function in code which created the project logs event
    """
    caller_filename: NotRequired[str]
    """
    Name of the file in code where the project logs event was created
    """
    caller_lineno: NotRequired[int]
    """
    Line of code where the project logs event was created
    """


class SpanAttributes(TypedDict):
    name: NotRequired[str]
    """
    Name of the span, for display purposes only
    """
    type: NotRequired[Literal["llm", "score", "function", "eval", "task", "tool"]]
    """
    Type of the span, for display purposes only
    """


class InsertProjectLogsEventReplace(TypedDict):
    input: NotRequired[Any]
    """
    The arguments that uniquely define a user input (an arbitrary, JSON serializable object).
    """
    output: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
    """
    error: NotRequired[Any]
    """
    The error that occurred, if any.
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    metrics: NotRequired[Metrics]
    """
    Metrics are numerical measurements tracking the execution of the code that produced the project logs event. Use "start" and "end" to track the time span over which the project logs event was produced
    """
    context: NotRequired[Context]
    """
    Context is additional information about the code that produced the project logs event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the project logs event
    """
    span_attributes: NotRequired[SpanAttributes]
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: NotRequired[str]
    """
    A unique identifier for the project logs event. If you don't provide one, BrainTrust will generate one for you
    """
    created: NotRequired[str]
    """
    The timestamp the project logs event was created
    """
    _object_delete: NotRequired[bool]
    """
    Pass `_object_delete=true` to mark the project logs event deleted. Deleted events will not show up in subsequent fetches for this project logs
    """
    _is_merge: NotRequired[Literal[False]]
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _parent_id: NotRequired[str]
    """
    Use the `_parent_id` field to create this row as a subspan of an existing row. It cannot be specified alongside `_is_merge=true`. Tracking hierarchical relationships are important for tracing (see the [guide](https://www.braintrust.dev/docs/guides/tracing) for full details).

    For example, say we have logged a row `{"id": "abc", "input": "foo", "output": "bar", "expected": "boo", "scores": {"correctness": 0.33}}`. We can create a sub-span of the parent row by logging `{"_parent_id": "abc", "id": "llm_call", "input": {"prompt": "What comes after foo?"}, "output": "bar", "metrics": {"tokens": 1}}`. In the webapp, only the root span row `"abc"` will show up in the summary view. You can view the full trace hierarchy (in this case, the `"llm_call"` row) by clicking on the "abc" row.
    """


class InsertProjectLogsEventMerge(TypedDict):
    input: NotRequired[Any]
    """
    The arguments that uniquely define a user input (an arbitrary, JSON serializable object).
    """
    output: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
    """
    error: NotRequired[Any]
    """
    The error that occurred, if any.
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    metrics: NotRequired[Metrics]
    """
    Metrics are numerical measurements tracking the execution of the code that produced the project logs event. Use "start" and "end" to track the time span over which the project logs event was produced
    """
    context: NotRequired[Context]
    """
    Context is additional information about the code that produced the project logs event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the project logs event
    """
    span_attributes: NotRequired[SpanAttributes]
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: NotRequired[str]
    """
    A unique identifier for the project logs event. If you don't provide one, BrainTrust will generate one for you
    """
    created: NotRequired[str]
    """
    The timestamp the project logs event was created
    """
    _object_delete: NotRequired[bool]
    """
    Pass `_object_delete=true` to mark the project logs event deleted. Deleted events will not show up in subsequent fetches for this project logs
    """
    _is_merge: Literal[True]
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _merge_paths: NotRequired[Sequence[Sequence[str]]]
    """
    The `_merge_paths` field allows controlling the depth of the merge. It can only be specified alongside `_is_merge=true`. `_merge_paths` is a list of paths, where each path is a list of field names. The deep merge will not descend below any of the specified merge paths.

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": {"b": 10}, "c": {"d": 20}}, "output": {"a": 20}}`. If we merge a new row as `{"_is_merge": true, "_merge_paths": [["input", "a"], ["output"]], "input": {"a": {"q": 30}, "c": {"e": 30}, "bar": "baz"}, "output": {"d": 40}}`, the new row will be `{"id": "foo": "input": {"a": {"q": 30}, "c": {"d": 20, "e": 30}, "bar": "baz"}, "output": {"d": 40}}`. In this case, due to the merge paths, we have replaced `input.a` and `output`, but have still deep-merged `input` and `input.c`.
    """


InsertProjectLogsEvent = Union[InsertProjectLogsEventReplace, InsertProjectLogsEventMerge]


class InsertProjectLogsEventRequest(TypedDict):
    events: Sequence[InsertProjectLogsEvent]
    """
    A list of project logs events to insert
    """


class Origin(TypedDict):
    object_type: Union[
        Literal["experiment", "dataset", "prompt", "function", "prompt_session"], Literal["project_logs"]
    ]
    """
    Type of the object the event is originating from.
    """
    object_id: str
    """
    ID of the object the event is originating from.
    """
    id: str
    """
    ID of the original event.
    """
    _xact_id: str
    """
    Transaction ID of the original event.
    """


class ProjectLogsEvent(TypedDict):
    id: str
    """
    A unique identifier for the project logs event. If you don't provide one, BrainTrust will generate one for you
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the project logs (see the `version` parameter)
    """
    created: str
    """
    The timestamp the project logs event was created
    """
    org_id: str
    """
    Unique id for the organization that the project belongs under
    """
    project_id: str
    """
    Unique identifier for the project
    """
    log_id: Literal["g"]
    """
    A literal 'g' which identifies the log as a project log
    """
    input: NotRequired[Any]
    """
    The arguments that uniquely define a user input (an arbitrary, JSON serializable object).
    """
    output: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
    """
    error: NotRequired[Any]
    """
    The error that occurred, if any.
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    metrics: NotRequired[Metrics]
    """
    Metrics are numerical measurements tracking the execution of the code that produced the project logs event. Use "start" and "end" to track the time span over which the project logs event was produced
    """
    context: NotRequired[Context]
    """
    Context is additional information about the code that produced the project logs event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the project logs event
    """
    span_id: str
    """
    A unique identifier used to link different project logs events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    span_parents: NotRequired[Sequence[str]]
    """
    An array of the parent `span_ids` of this project logs event. This should be empty for the root span of a trace, and should most often contain just one parent element for subspans
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this project logs event belongs to
    """
    is_root: NotRequired[bool]
    """
    Whether this span is a root span
    """
    span_attributes: NotRequired[SpanAttributes]
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    origin: NotRequired[Origin]
    """
    Indicates the event was copied from another object.
    """


class FetchProjectLogsEventsResponse(TypedDict):
    events: Sequence[ProjectLogsEvent]
    """
    A list of fetched events
    """
    cursor: NotRequired[str]
    """
    Pagination cursor

    Pass this string directly as the `cursor` param to your next fetch request to get the next page of results. Not provided if the returned result set is empty.
    """


FetchLimit = Optional[int]


FetchPaginationCursor = Optional[str]


class PathLookupFilter(TypedDict):
    type: Literal["path_lookup"]
    """
    Denotes the type of filter as a path-lookup filter
    """
    path: Sequence[str]
    """
    List of fields describing the path to the value to be checked against. For instance, if you wish to filter on the value of `c` in `{"input": {"a": {"b": {"c": "hello"}}}}`, pass `path=["input", "a", "b", "c"]`
    """
    value: NotRequired[Any]
    """
    The value to compare equality-wise against the event value at the specified `path`. The value must be a "primitive", that is, any JSON-serializable object except for objects and arrays. For instance, if you wish to filter on the value of "input.a.b.c" in the object `{"input": {"a": {"b": {"c": "hello"}}}}`, pass `value="hello"`
    """


FetchEventsFilters = Optional[Sequence[PathLookupFilter]]


class FetchEventsRequest(TypedDict):
    limit: NotRequired[FetchLimit]
    cursor: NotRequired[FetchPaginationCursor]
    max_xact_id: NotRequired[MaxXactId]
    max_root_span_id: NotRequired[MaxRootSpanId]
    filters: NotRequired[FetchEventsFilters]
    version: NotRequired[Version]


class FeedbackResponseSchema(TypedDict):
    status: Literal["success"]


class FeedbackProjectLogsItem(TypedDict):
    id: str
    """
    The id of the project logs event to log feedback for. This is the row `id` returned by `POST /v1/project_logs/{project_id}/insert`
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the project logs event
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not
    """
    comment: NotRequired[str]
    """
    An optional comment string to log about the project logs event
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
    """
    source: NotRequired[Literal["app", "api", "external"]]
    """
    The source of the feedback. Must be one of "external" (default), "app", or "api"
    """


class FeedbackProjectLogsEventRequest(TypedDict):
    feedback: Sequence[FeedbackProjectLogsItem]
    """
    A list of project logs feedback items
    """


class RepoInfo(TypedDict):
    commit: NotRequired[str]
    """
    SHA of most recent commit
    """
    branch: NotRequired[str]
    """
    Name of the branch the most recent commit belongs to
    """
    tag: NotRequired[str]
    """
    Name of the tag on the most recent commit
    """
    dirty: NotRequired[bool]
    """
    Whether or not the repo had uncommitted changes when snapshotted
    """
    author_name: NotRequired[str]
    """
    Name of the author of the most recent commit
    """
    author_email: NotRequired[str]
    """
    Email of the author of the most recent commit
    """
    commit_message: NotRequired[str]
    """
    Most recent commit message
    """
    commit_time: NotRequired[str]
    """
    Time of the most recent commit
    """
    git_diff: NotRequired[str]
    """
    If the repo was dirty when run, this includes the diff between the current state of the repo and the most recent commit.
    """


class Experiment(TypedDict):
    id: str
    """
    Unique identifier for the experiment
    """
    project_id: str
    """
    Unique identifier for the project that the experiment belongs under
    """
    name: str
    """
    Name of the experiment. Within a project, experiment names are unique
    """
    description: NotRequired[str]
    """
    Textual description of the experiment
    """
    created: NotRequired[str]
    """
    Date of experiment creation
    """
    repo_info: NotRequired[RepoInfo]
    commit: NotRequired[str]
    """
    Commit, taken directly from `repo_info.commit`
    """
    base_exp_id: NotRequired[str]
    """
    Id of default base experiment to compare against when viewing this experiment
    """
    deleted_at: NotRequired[str]
    """
    Date of experiment deletion, or null if the experiment is still active
    """
    dataset_id: NotRequired[str]
    """
    Identifier of the linked dataset, or null if the experiment is not linked to a dataset
    """
    dataset_version: NotRequired[str]
    """
    Version number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.
    """
    public: bool
    """
    Whether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization
    """
    user_id: NotRequired[str]
    """
    Identifies the user who created the experiment
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the experiment
    """


class CreateExperiment(TypedDict):
    project_id: str
    """
    Unique identifier for the project that the experiment belongs under
    """
    name: NotRequired[str]
    """
    Name of the experiment. Within a project, experiment names are unique
    """
    description: NotRequired[str]
    """
    Textual description of the experiment
    """
    repo_info: NotRequired[RepoInfo]
    base_exp_id: NotRequired[str]
    """
    Id of default base experiment to compare against when viewing this experiment
    """
    dataset_id: NotRequired[str]
    """
    Identifier of the linked dataset, or null if the experiment is not linked to a dataset
    """
    dataset_version: NotRequired[str]
    """
    Version number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.
    """
    public: NotRequired[bool]
    """
    Whether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the experiment
    """
    ensure_new: NotRequired[bool]
    """
    Normally, creating an experiment with the same name as an existing experiment will return the existing one un-modified. But if `ensure_new` is true, registration will generate a new experiment with a unique name in case of a conflict.
    """


class PatchExperiment(TypedDict):
    name: NotRequired[str]
    """
    Name of the experiment. Within a project, experiment names are unique
    """
    description: NotRequired[str]
    """
    Textual description of the experiment
    """
    repo_info: NotRequired[RepoInfo]
    base_exp_id: NotRequired[str]
    """
    Id of default base experiment to compare against when viewing this experiment
    """
    dataset_id: NotRequired[str]
    """
    Identifier of the linked dataset, or null if the experiment is not linked to a dataset
    """
    dataset_version: NotRequired[str]
    """
    Version number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.
    """
    public: NotRequired[bool]
    """
    Whether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the experiment
    """


class Metrics3(TypedDict):
    start: NotRequired[float]
    """
    A unix timestamp recording when the section of code which produced the experiment event started
    """
    end: NotRequired[float]
    """
    A unix timestamp recording when the section of code which produced the experiment event finished
    """
    prompt_tokens: NotRequired[int]
    """
    The number of tokens in the prompt used to generate the experiment event (only set if this is an LLM span)
    """
    completion_tokens: NotRequired[int]
    """
    The number of tokens in the completion generated by the model (only set if this is an LLM span)
    """
    tokens: NotRequired[int]
    """
    The total number of tokens in the input and output of the experiment event.
    """


class Context3(TypedDict):
    caller_functionname: NotRequired[str]
    """
    The function in code which created the experiment event
    """
    caller_filename: NotRequired[str]
    """
    Name of the file in code where the experiment event was created
    """
    caller_lineno: NotRequired[int]
    """
    Line of code where the experiment event was created
    """


class InsertExperimentEventReplace(TypedDict):
    input: NotRequired[Any]
    """
    The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical
    """
    output: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models
    """
    error: NotRequired[Any]
    """
    The error that occurred, if any.
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    metrics: NotRequired[Metrics3]
    """
    Metrics are numerical measurements tracking the execution of the code that produced the experiment event. Use "start" and "end" to track the time span over which the experiment event was produced
    """
    context: NotRequired[Context3]
    """
    Context is additional information about the code that produced the experiment event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the experiment event
    """
    span_attributes: NotRequired[SpanAttributes]
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: NotRequired[str]
    """
    A unique identifier for the experiment event. If you don't provide one, BrainTrust will generate one for you
    """
    dataset_record_id: NotRequired[str]
    """
    If the experiment is associated to a dataset, this is the event-level dataset id this experiment event is tied to
    """
    created: NotRequired[str]
    """
    The timestamp the experiment event was created
    """
    _object_delete: NotRequired[bool]
    """
    Pass `_object_delete=true` to mark the experiment event deleted. Deleted events will not show up in subsequent fetches for this experiment
    """
    _is_merge: NotRequired[Literal[False]]
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _parent_id: NotRequired[str]
    """
    Use the `_parent_id` field to create this row as a subspan of an existing row. It cannot be specified alongside `_is_merge=true`. Tracking hierarchical relationships are important for tracing (see the [guide](https://www.braintrust.dev/docs/guides/tracing) for full details).

    For example, say we have logged a row `{"id": "abc", "input": "foo", "output": "bar", "expected": "boo", "scores": {"correctness": 0.33}}`. We can create a sub-span of the parent row by logging `{"_parent_id": "abc", "id": "llm_call", "input": {"prompt": "What comes after foo?"}, "output": "bar", "metrics": {"tokens": 1}}`. In the webapp, only the root span row `"abc"` will show up in the summary view. You can view the full trace hierarchy (in this case, the `"llm_call"` row) by clicking on the "abc" row.
    """


class InsertExperimentEventMerge(TypedDict):
    input: NotRequired[Any]
    """
    The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical
    """
    output: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models
    """
    error: NotRequired[Any]
    """
    The error that occurred, if any.
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    metrics: NotRequired[Metrics3]
    """
    Metrics are numerical measurements tracking the execution of the code that produced the experiment event. Use "start" and "end" to track the time span over which the experiment event was produced
    """
    context: NotRequired[Context3]
    """
    Context is additional information about the code that produced the experiment event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the experiment event
    """
    span_attributes: NotRequired[SpanAttributes]
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: NotRequired[str]
    """
    A unique identifier for the experiment event. If you don't provide one, BrainTrust will generate one for you
    """
    dataset_record_id: NotRequired[str]
    """
    If the experiment is associated to a dataset, this is the event-level dataset id this experiment event is tied to
    """
    created: NotRequired[str]
    """
    The timestamp the experiment event was created
    """
    _object_delete: NotRequired[bool]
    """
    Pass `_object_delete=true` to mark the experiment event deleted. Deleted events will not show up in subsequent fetches for this experiment
    """
    _is_merge: Literal[True]
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _merge_paths: NotRequired[Sequence[Sequence[str]]]
    """
    The `_merge_paths` field allows controlling the depth of the merge. It can only be specified alongside `_is_merge=true`. `_merge_paths` is a list of paths, where each path is a list of field names. The deep merge will not descend below any of the specified merge paths.

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": {"b": 10}, "c": {"d": 20}}, "output": {"a": 20}}`. If we merge a new row as `{"_is_merge": true, "_merge_paths": [["input", "a"], ["output"]], "input": {"a": {"q": 30}, "c": {"e": 30}, "bar": "baz"}, "output": {"d": 40}}`, the new row will be `{"id": "foo": "input": {"a": {"q": 30}, "c": {"d": 20, "e": 30}, "bar": "baz"}, "output": {"d": 40}}`. In this case, due to the merge paths, we have replaced `input.a` and `output`, but have still deep-merged `input` and `input.c`.
    """


InsertExperimentEvent = Union[InsertExperimentEventReplace, InsertExperimentEventMerge]


class InsertExperimentEventRequest(TypedDict):
    events: Sequence[InsertExperimentEvent]
    """
    A list of experiment events to insert
    """


class ExperimentEvent(TypedDict):
    id: str
    """
    A unique identifier for the experiment event. If you don't provide one, BrainTrust will generate one for you
    """
    dataset_record_id: NotRequired[str]
    """
    If the experiment is associated to a dataset, this is the event-level dataset id this experiment event is tied to
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the experiment (see the `version` parameter)
    """
    created: str
    """
    The timestamp the experiment event was created
    """
    project_id: str
    """
    Unique identifier for the project that the experiment belongs under
    """
    experiment_id: str
    """
    Unique identifier for the experiment
    """
    input: NotRequired[Any]
    """
    The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical
    """
    output: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models
    """
    error: NotRequired[Any]
    """
    The error that occurred, if any.
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    metrics: NotRequired[Metrics3]
    """
    Metrics are numerical measurements tracking the execution of the code that produced the experiment event. Use "start" and "end" to track the time span over which the experiment event was produced
    """
    context: NotRequired[Context3]
    """
    Context is additional information about the code that produced the experiment event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the experiment event
    """
    span_id: str
    """
    A unique identifier used to link different experiment events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    span_parents: NotRequired[Sequence[str]]
    """
    An array of the parent `span_ids` of this experiment event. This should be empty for the root span of a trace, and should most often contain just one parent element for subspans
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this experiment event belongs to
    """
    span_attributes: NotRequired[SpanAttributes]
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    is_root: NotRequired[bool]
    """
    Whether this span is a root span
    """
    origin: NotRequired[Origin]
    """
    Indicates the event was copied from another object.
    """


class FetchExperimentEventsResponse(TypedDict):
    events: Sequence[ExperimentEvent]
    """
    A list of fetched events
    """
    cursor: NotRequired[str]
    """
    Pagination cursor

    Pass this string directly as the `cursor` param to your next fetch request to get the next page of results. Not provided if the returned result set is empty.
    """


class FeedbackExperimentItem(TypedDict):
    id: str
    """
    The id of the experiment event to log feedback for. This is the row `id` returned by `POST /v1/experiment/{experiment_id}/insert`
    """
    scores: NotRequired[Mapping[str, float]]
    """
    A dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the experiment event
    """
    expected: NotRequired[Any]
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not
    """
    comment: NotRequired[str]
    """
    An optional comment string to log about the experiment event
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
    """
    source: NotRequired[Literal["app", "api", "external"]]
    """
    The source of the feedback. Must be one of "external" (default), "app", or "api"
    """


class FeedbackExperimentEventRequest(TypedDict):
    feedback: Sequence[FeedbackExperimentItem]
    """
    A list of experiment feedback items
    """


class ScoreSummary(TypedDict):
    name: str
    """
    Name of the score
    """
    score: float
    """
    Average score across all examples
    """
    diff: NotRequired[float]
    """
    Difference in score between the current and comparison experiment
    """
    improvements: int
    """
    Number of improvements in the score
    """
    regressions: int
    """
    Number of regressions in the score
    """


class MetricSummary(TypedDict):
    name: str
    """
    Name of the metric
    """
    metric: float
    """
    Average metric across all examples
    """
    unit: str
    """
    Unit label for the metric
    """
    diff: NotRequired[float]
    """
    Difference in metric between the current and comparison experiment
    """
    improvements: int
    """
    Number of improvements in the metric
    """
    regressions: int
    """
    Number of regressions in the metric
    """


class SummarizeExperimentResponse(TypedDict):
    project_name: str
    """
    Name of the project that the experiment belongs to
    """
    experiment_name: str
    """
    Name of the experiment
    """
    project_url: str
    """
    URL to the project's page in the Braintrust app
    """
    experiment_url: str
    """
    URL to the experiment's page in the Braintrust app
    """
    comparison_experiment_name: NotRequired[str]
    """
    The experiment which scores are baselined against
    """
    scores: NotRequired[Mapping[str, ScoreSummary]]
    """
    Summary of the experiment's scores
    """
    metrics: NotRequired[Mapping[str, MetricSummary]]
    """
    Summary of the experiment's metrics
    """


class Dataset(TypedDict):
    id: str
    """
    Unique identifier for the dataset
    """
    project_id: str
    """
    Unique identifier for the project that the dataset belongs under
    """
    name: str
    """
    Name of the dataset. Within a project, dataset names are unique
    """
    description: NotRequired[str]
    """
    Textual description of the dataset
    """
    created: NotRequired[str]
    """
    Date of dataset creation
    """
    deleted_at: NotRequired[str]
    """
    Date of dataset deletion, or null if the dataset is still active
    """
    user_id: NotRequired[str]
    """
    Identifies the user who created the dataset
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the dataset
    """


class CreateDataset(TypedDict):
    project_id: str
    """
    Unique identifier for the project that the dataset belongs under
    """
    name: str
    """
    Name of the dataset. Within a project, dataset names are unique
    """
    description: NotRequired[str]
    """
    Textual description of the dataset
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the dataset
    """


class PatchDataset(TypedDict):
    name: NotRequired[str]
    """
    Name of the dataset. Within a project, dataset names are unique
    """
    description: NotRequired[str]
    """
    Textual description of the dataset
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the dataset
    """


class InsertDatasetEventReplace(TypedDict):
    input: NotRequired[Any]
    """
    The argument that uniquely define an input case (an arbitrary, JSON serializable object)
    """
    expected: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object)
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    id: NotRequired[str]
    """
    A unique identifier for the dataset event. If you don't provide one, BrainTrust will generate one for you
    """
    created: NotRequired[str]
    """
    The timestamp the dataset event was created
    """
    _object_delete: NotRequired[bool]
    """
    Pass `_object_delete=true` to mark the dataset event deleted. Deleted events will not show up in subsequent fetches for this dataset
    """
    _is_merge: NotRequired[Literal[False]]
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _parent_id: NotRequired[str]
    """
    Use the `_parent_id` field to create this row as a subspan of an existing row. It cannot be specified alongside `_is_merge=true`. Tracking hierarchical relationships are important for tracing (see the [guide](https://www.braintrust.dev/docs/guides/tracing) for full details).

    For example, say we have logged a row `{"id": "abc", "input": "foo", "output": "bar", "expected": "boo", "scores": {"correctness": 0.33}}`. We can create a sub-span of the parent row by logging `{"_parent_id": "abc", "id": "llm_call", "input": {"prompt": "What comes after foo?"}, "output": "bar", "metrics": {"tokens": 1}}`. In the webapp, only the root span row `"abc"` will show up in the summary view. You can view the full trace hierarchy (in this case, the `"llm_call"` row) by clicking on the "abc" row.
    """


class InsertDatasetEventMerge(TypedDict):
    input: NotRequired[Any]
    """
    The argument that uniquely define an input case (an arbitrary, JSON serializable object)
    """
    expected: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object)
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    id: NotRequired[str]
    """
    A unique identifier for the dataset event. If you don't provide one, BrainTrust will generate one for you
    """
    created: NotRequired[str]
    """
    The timestamp the dataset event was created
    """
    _object_delete: NotRequired[bool]
    """
    Pass `_object_delete=true` to mark the dataset event deleted. Deleted events will not show up in subsequent fetches for this dataset
    """
    _is_merge: Literal[True]
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _merge_paths: NotRequired[Sequence[Sequence[str]]]
    """
    The `_merge_paths` field allows controlling the depth of the merge. It can only be specified alongside `_is_merge=true`. `_merge_paths` is a list of paths, where each path is a list of field names. The deep merge will not descend below any of the specified merge paths.

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": {"b": 10}, "c": {"d": 20}}, "output": {"a": 20}}`. If we merge a new row as `{"_is_merge": true, "_merge_paths": [["input", "a"], ["output"]], "input": {"a": {"q": 30}, "c": {"e": 30}, "bar": "baz"}, "output": {"d": 40}}`, the new row will be `{"id": "foo": "input": {"a": {"q": 30}, "c": {"d": 20, "e": 30}, "bar": "baz"}, "output": {"d": 40}}`. In this case, due to the merge paths, we have replaced `input.a` and `output`, but have still deep-merged `input` and `input.c`.
    """


InsertDatasetEvent = Union[InsertDatasetEventReplace, InsertDatasetEventMerge]


class InsertDatasetEventRequest(TypedDict):
    events: Sequence[InsertDatasetEvent]
    """
    A list of dataset events to insert
    """


class DatasetEvent(TypedDict):
    id: str
    """
    A unique identifier for the dataset event. If you don't provide one, BrainTrust will generate one for you
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the dataset (see the `version` parameter)
    """
    created: str
    """
    The timestamp the dataset event was created
    """
    project_id: str
    """
    Unique identifier for the project that the dataset belongs under
    """
    dataset_id: str
    """
    Unique identifier for the dataset
    """
    input: NotRequired[Any]
    """
    The argument that uniquely define an input case (an arbitrary, JSON serializable object)
    """
    expected: NotRequired[Any]
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object)
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: NotRequired[Sequence[str]]
    """
    A list of tags to log
    """
    span_id: str
    """
    A unique identifier used to link different dataset events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this dataset event belongs to
    """
    is_root: NotRequired[bool]
    """
    Whether this span is a root span
    """
    origin: NotRequired[Origin]
    """
    Indicates the event was copied from another object.
    """


class FetchDatasetEventsResponse(TypedDict):
    events: Sequence[DatasetEvent]
    """
    A list of fetched events
    """
    cursor: NotRequired[str]
    """
    Pagination cursor

    Pass this string directly as the `cursor` param to your next fetch request to get the next page of results. Not provided if the returned result set is empty.
    """


class FeedbackDatasetItem(TypedDict):
    id: str
    """
    The id of the dataset event to log feedback for. This is the row `id` returned by `POST /v1/dataset/{dataset_id}/insert`
    """
    comment: NotRequired[str]
    """
    An optional comment string to log about the dataset event
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    A dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
    """
    source: NotRequired[Literal["app", "api", "external"]]
    """
    The source of the feedback. Must be one of "external" (default), "app", or "api"
    """


class FeedbackDatasetEventRequest(TypedDict):
    feedback: Sequence[FeedbackDatasetItem]
    """
    A list of dataset feedback items
    """


class DataSummary(TypedDict):
    total_records: int
    """
    Total number of records in the dataset
    """


class SummarizeDatasetResponse(TypedDict):
    project_name: str
    """
    Name of the project that the dataset belongs to
    """
    dataset_name: str
    """
    Name of the dataset
    """
    project_url: str
    """
    URL to the project's page in the Braintrust app
    """
    dataset_url: str
    """
    URL to the dataset's page in the Braintrust app
    """
    data_summary: NotRequired[DataSummary]


class ChatCompletionContentPartText(TypedDict):
    text: NotRequired[str]
    type: Literal["text"]


class ImageUrl(TypedDict):
    url: str
    detail: NotRequired[Union[Literal["auto"], Literal["low"], Literal["high"]]]


class ChatCompletionContentPartImage(TypedDict):
    image_url: ImageUrl
    type: Literal["image_url"]


ChatCompletionContentPart = Union[ChatCompletionContentPartText, ChatCompletionContentPartImage]


ChatCompletionContent = Union[str, Sequence[ChatCompletionContentPart]]


class Function(TypedDict):
    arguments: str
    name: str


class ChatCompletionMessageToolCall(TypedDict):
    id: str
    function: Function
    type: Literal["function"]


class ChatCompletionMessageParam1(TypedDict):
    content: NotRequired[str]
    role: Literal["system"]
    name: NotRequired[str]


class ChatCompletionMessageParam2(TypedDict):
    content: NotRequired[ChatCompletionContent]
    role: Literal["user"]
    name: NotRequired[str]


class FunctionCall(TypedDict):
    arguments: str
    name: str


class ChatCompletionMessageParam3(TypedDict):
    role: Literal["assistant"]
    content: NotRequired[str]
    function_call: NotRequired[FunctionCall]
    name: NotRequired[str]
    tool_calls: NotRequired[Sequence[ChatCompletionMessageToolCall]]


class ChatCompletionMessageParam4(TypedDict):
    content: NotRequired[str]
    role: Literal["tool"]
    tool_call_id: NotRequired[str]


class ChatCompletionMessageParam5(TypedDict):
    content: NotRequired[str]
    name: str
    role: Literal["function"]


class ChatCompletionMessageParam6(TypedDict):
    role: Literal["model"]
    content: NotRequired[str]


ChatCompletionMessageParam = Union[
    ChatCompletionMessageParam1,
    ChatCompletionMessageParam2,
    ChatCompletionMessageParam3,
    ChatCompletionMessageParam4,
    ChatCompletionMessageParam5,
    ChatCompletionMessageParam6,
]


class ResponseFormat(TypedDict):
    type: Literal["json_object"]


class JsonSchema(TypedDict):
    name: str
    description: NotRequired[str]
    schema_: NotRequired[Mapping[str, Any]]
    strict: NotRequired[bool]


class ResponseFormat1(TypedDict):
    type: Literal["json_schema"]
    json_schema: JsonSchema


class ResponseFormat2(TypedDict):
    type: Literal["text"]


class Function1(TypedDict):
    name: str


class ToolChoice(TypedDict):
    type: Literal["function"]
    function: Function1


class FunctionCall1(TypedDict):
    name: str


class ModelParams1(TypedDict):
    use_cache: NotRequired[bool]
    temperature: NotRequired[float]
    top_p: NotRequired[float]
    max_tokens: NotRequired[float]
    frequency_penalty: NotRequired[float]
    presence_penalty: NotRequired[float]
    response_format: NotRequired[Union[ResponseFormat, ResponseFormat1, ResponseFormat2, Mapping[str, Any]]]
    tool_choice: NotRequired[Union[Literal["auto"], Literal["none"], Literal["required"], ToolChoice]]
    function_call: NotRequired[Union[Literal["auto"], Literal["none"], FunctionCall1]]
    n: NotRequired[float]
    stop: NotRequired[Sequence[str]]


class ModelParams2(TypedDict):
    use_cache: NotRequired[bool]
    max_tokens: float
    temperature: float
    top_p: NotRequired[float]
    top_k: NotRequired[float]
    stop_sequences: NotRequired[Sequence[str]]
    max_tokens_to_sample: NotRequired[float]
    """
    This is a legacy parameter that should not be used.
    """


class ModelParams3(TypedDict):
    use_cache: NotRequired[bool]
    temperature: NotRequired[float]
    maxOutputTokens: NotRequired[float]
    topP: NotRequired[float]
    topK: NotRequired[float]


class ModelParams4(TypedDict):
    use_cache: NotRequired[bool]
    temperature: NotRequired[float]
    topK: NotRequired[float]


class ModelParams5(TypedDict):
    use_cache: NotRequired[bool]


ModelParams = Union[ModelParams1, ModelParams2, ModelParams3, ModelParams4, ModelParams5]


class SavedFunctionId1(TypedDict):
    type: Literal["function"]
    id: str


class SavedFunctionId2(TypedDict):
    type: Literal["global"]
    name: str


SavedFunctionId = Union[SavedFunctionId1, SavedFunctionId2]


class Prompt(TypedDict):
    type: Literal["completion"]
    content: str


class Prompt1(TypedDict):
    type: Literal["chat"]
    messages: Sequence[ChatCompletionMessageParam]
    tools: NotRequired[str]


class Options(TypedDict):
    model: NotRequired[str]
    params: NotRequired[ModelParams]
    position: NotRequired[str]


class Parser(TypedDict):
    type: Literal["llm_classifier"]
    use_cot: bool
    choice_scores: Mapping[str, float]


class Origin3(TypedDict):
    prompt_id: NotRequired[str]
    project_id: NotRequired[str]
    prompt_version: NotRequired[str]


class PromptData(TypedDict):
    prompt: NotRequired[Union[Prompt, Prompt1, Mapping[str, Any]]]
    options: NotRequired[Options]
    parser: NotRequired[Parser]
    tool_functions: NotRequired[Sequence[SavedFunctionId]]
    origin: NotRequired[Origin3]


class Prompt2(TypedDict):
    id: str
    """
    Unique identifier for the prompt
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the prompt (see the `version` parameter)
    """
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    log_id: Literal["p"]
    """
    A literal 'p' which identifies the object as a project prompt
    """
    org_id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    description: NotRequired[str]
    """
    Textual description of the prompt
    """
    created: NotRequired[str]
    """
    Date of prompt creation
    """
    prompt_data: NotRequired[PromptData]
    tags: NotRequired[Sequence[str]]
    """
    A list of tags for the prompt
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the prompt
    """
    function_type: NotRequired[Literal["llm", "scorer", "task", "tool"]]


class CreatePrompt(TypedDict):
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    description: NotRequired[str]
    """
    Textual description of the prompt
    """
    prompt_data: NotRequired[PromptData]
    tags: NotRequired[Sequence[str]]
    """
    A list of tags for the prompt
    """
    function_type: NotRequired[Literal["llm", "scorer", "task", "tool"]]


class PatchPrompt(TypedDict):
    name: NotRequired[str]
    """
    Name of the prompt
    """
    slug: NotRequired[str]
    """
    Unique identifier for the prompt
    """
    description: NotRequired[str]
    """
    Textual description of the prompt
    """
    prompt_data: NotRequired[PromptData]
    tags: NotRequired[Sequence[str]]
    """
    A list of tags for the prompt
    """


Permission = Literal["create", "read", "update", "delete", "create_acls", "read_acls", "update_acls", "delete_acls"]


class MemberPermission(TypedDict):
    permission: Permission
    restrict_object_type: NotRequired[AclObjectType]


class Role(TypedDict):
    id: str
    """
    Unique identifier for the role
    """
    org_id: NotRequired[str]
    """
    Unique id for the organization that the role belongs under

    A null org_id indicates a system role, which may be assigned to anybody and inherited by any other role, but cannot be edited.

    It is forbidden to change the org after creating a role
    """
    user_id: NotRequired[str]
    """
    Identifies the user who created the role
    """
    created: NotRequired[str]
    """
    Date of role creation
    """
    name: str
    """
    Name of the role
    """
    description: NotRequired[str]
    """
    Textual description of the role
    """
    deleted_at: NotRequired[str]
    """
    Date of role deletion, or null if the role is still active
    """
    member_permissions: NotRequired[Sequence[MemberPermission]]
    """
    (permission, restrict_object_type) tuples which belong to this role
    """
    member_roles: NotRequired[Sequence[str]]
    """
    Ids of the roles this role inherits from

    An inheriting role has all the permissions contained in its member roles, as well as all of their inherited permissions
    """


class CreateRole(TypedDict):
    name: str
    """
    Name of the role
    """
    description: NotRequired[str]
    """
    Textual description of the role
    """
    member_permissions: NotRequired[Sequence[MemberPermission]]
    """
    (permission, restrict_object_type) tuples which belong to this role
    """
    member_roles: NotRequired[Sequence[str]]
    """
    Ids of the roles this role inherits from

    An inheriting role has all the permissions contained in its member roles, as well as all of their inherited permissions
    """
    org_name: NotRequired[str]
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the role belongs in.
    """


class AddMemberPermission(TypedDict):
    permission: Permission
    restrict_object_type: NotRequired[AclObjectType]


class RemoveMemberPermission(TypedDict):
    permission: Permission
    restrict_object_type: NotRequired[AclObjectType]


class PatchRole(TypedDict):
    description: NotRequired[str]
    """
    Textual description of the role
    """
    name: NotRequired[str]
    """
    Name of the role
    """
    add_member_permissions: NotRequired[Sequence[AddMemberPermission]]
    """
    A list of permissions to add to the role
    """
    remove_member_permissions: NotRequired[Sequence[RemoveMemberPermission]]
    """
    A list of permissions to remove from the role
    """
    add_member_roles: NotRequired[Sequence[str]]
    """
    A list of role IDs to add to the role's inheriting-from set
    """
    remove_member_roles: NotRequired[Sequence[str]]
    """
    A list of role IDs to remove from the role's inheriting-from set
    """


class Group(TypedDict):
    id: str
    """
    Unique identifier for the group
    """
    org_id: str
    """
    Unique id for the organization that the group belongs under

    It is forbidden to change the org after creating a group
    """
    user_id: NotRequired[str]
    """
    Identifies the user who created the group
    """
    created: NotRequired[str]
    """
    Date of group creation
    """
    name: str
    """
    Name of the group
    """
    description: NotRequired[str]
    """
    Textual description of the group
    """
    deleted_at: NotRequired[str]
    """
    Date of group deletion, or null if the group is still active
    """
    member_users: NotRequired[Sequence[str]]
    """
    Ids of users which belong to this group
    """
    member_groups: NotRequired[Sequence[str]]
    """
    Ids of the groups this group inherits from

    An inheriting group has all the users contained in its member groups, as well as all of their inherited users
    """


class CreateGroup(TypedDict):
    name: str
    """
    Name of the group
    """
    description: NotRequired[str]
    """
    Textual description of the group
    """
    member_users: NotRequired[Sequence[str]]
    """
    Ids of users which belong to this group
    """
    member_groups: NotRequired[Sequence[str]]
    """
    Ids of the groups this group inherits from

    An inheriting group has all the users contained in its member groups, as well as all of their inherited users
    """
    org_name: NotRequired[str]
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the group belongs in.
    """


class PatchGroup(TypedDict):
    description: NotRequired[str]
    """
    Textual description of the group
    """
    name: NotRequired[str]
    """
    Name of the group
    """
    add_member_users: NotRequired[Sequence[str]]
    """
    A list of user IDs to add to the group
    """
    remove_member_users: NotRequired[Sequence[str]]
    """
    A list of user IDs to remove from the group
    """
    add_member_groups: NotRequired[Sequence[str]]
    """
    A list of group IDs to add to the group's inheriting-from set
    """
    remove_member_groups: NotRequired[Sequence[str]]
    """
    A list of group IDs to remove from the group's inheriting-from set
    """


class Acl(TypedDict):
    id: str
    """
    Unique identifier for the acl
    """
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the ACL applies to
    """
    user_id: NotRequired[str]
    """
    Id of the user the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    group_id: NotRequired[str]
    """
    Id of the group the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    permission: NotRequired[Permission]
    """
    Permission the ACL grants. Exactly one of `permission` and `role_id` will be provided
    """
    restrict_object_type: NotRequired[AclObjectType]
    """
    When setting a permission directly, optionally restricts the permission grant to just the specified object type. Cannot be set alongside a `role_id`.
    """
    role_id: NotRequired[str]
    """
    Id of the role the ACL grants. Exactly one of `permission` and `role_id` will be provided
    """
    _object_org_id: str
    """
    The organization the ACL's referred object belongs to
    """
    created: NotRequired[str]
    """
    Date of acl creation
    """


class AclItem(TypedDict):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the ACL applies to
    """
    user_id: NotRequired[str]
    """
    Id of the user the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    group_id: NotRequired[str]
    """
    Id of the group the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    permission: NotRequired[Permission]
    """
    Permission the ACL grants. Exactly one of `permission` and `role_id` will be provided
    """
    restrict_object_type: NotRequired[AclObjectType]
    """
    When setting a permission directly, optionally restricts the permission grant to just the specified object type. Cannot be set alongside a `role_id`.
    """
    role_id: NotRequired[str]
    """
    Id of the role the ACL grants. Exactly one of `permission` and `role_id` will be provided
    """


class AclBatchUpdateResponse(TypedDict):
    added_acls: Sequence[Acl]
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """
    removed_acls: Sequence[Acl]
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """


class AclBatchUpdateRequest(TypedDict):
    add_acls: NotRequired[Sequence[AclItem]]
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """
    remove_acls: NotRequired[Sequence[AclItem]]
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """


class User(TypedDict):
    id: str
    """
    Unique identifier for the user
    """
    given_name: NotRequired[str]
    """
    Given name of the user
    """
    family_name: NotRequired[str]
    """
    Family name of the user
    """
    email: NotRequired[str]
    """
    The user's email
    """
    avatar_url: NotRequired[str]
    """
    URL of the user's Avatar image
    """
    created: NotRequired[str]
    """
    Date of user creation
    """


class ProjectScoreCategory(TypedDict):
    name: str
    """
    Name of the category
    """
    value: float
    """
    Numerical value of the category. Must be between 0 and 1, inclusive
    """


ProjectScoreCategories = Union[Sequence[ProjectScoreCategory], Mapping[str, float], Sequence[str], Mapping[str, Any]]


class OnlineScoreConfig(TypedDict):
    sampling_rate: float
    """
    The sampling rate for online scoring
    """
    scorers: Sequence[SavedFunctionId]
    """
    The list of scorers to use for online scoring
    """
    apply_to_root_span: NotRequired[bool]
    """
    Whether to trigger online scoring on the root span of each trace
    """
    apply_to_span_names: NotRequired[Sequence[str]]
    """
    Trigger online scoring on any spans with a name in this list
    """


class ProjectScoreConfig(TypedDict):
    multi_select: NotRequired[bool]
    destination: NotRequired[Literal["expected"]]
    online: NotRequired[OnlineScoreConfig]


class ProjectScore(TypedDict):
    id: str
    """
    Unique identifier for the project score
    """
    project_id: str
    """
    Unique identifier for the project that the project score belongs under
    """
    user_id: str
    created: NotRequired[str]
    """
    Date of project score creation
    """
    name: str
    """
    Name of the project score
    """
    description: NotRequired[str]
    """
    Textual description of the project score
    """
    score_type: ProjectScoreType
    categories: NotRequired[ProjectScoreCategories]
    config: NotRequired[ProjectScoreConfig]
    position: NotRequired[str]
    """
    An optional LexoRank-based string that sets the sort position for the score in the UI
    """


class CreateProjectScore(TypedDict):
    project_id: str
    """
    Unique identifier for the project that the project score belongs under
    """
    name: str
    """
    Name of the project score
    """
    description: NotRequired[str]
    """
    Textual description of the project score
    """
    score_type: ProjectScoreType
    categories: NotRequired[ProjectScoreCategories]
    config: NotRequired[ProjectScoreConfig]


class PatchProjectScore(TypedDict):
    name: NotRequired[str]
    """
    Name of the project score
    """
    description: NotRequired[str]
    """
    Textual description of the project score
    """
    score_type: NotRequired[ProjectScoreType]
    categories: NotRequired[ProjectScoreCategories]
    config: NotRequired[ProjectScoreConfig]


class ProjectTag(TypedDict):
    id: str
    """
    Unique identifier for the project tag
    """
    project_id: str
    """
    Unique identifier for the project that the project tag belongs under
    """
    user_id: str
    created: NotRequired[str]
    """
    Date of project tag creation
    """
    name: str
    """
    Name of the project tag
    """
    description: NotRequired[str]
    """
    Textual description of the project tag
    """
    color: NotRequired[str]
    """
    Color of the tag for the UI
    """


class CreateProjectTag(TypedDict):
    project_id: str
    """
    Unique identifier for the project that the project tag belongs under
    """
    name: str
    """
    Name of the project tag
    """
    description: NotRequired[str]
    """
    Textual description of the project tag
    """
    color: NotRequired[str]
    """
    Color of the tag for the UI
    """


class PatchProjectTag(TypedDict):
    name: NotRequired[str]
    """
    Name of the project tag
    """
    description: NotRequired[str]
    """
    Textual description of the project tag
    """
    color: NotRequired[str]
    """
    Color of the tag for the UI
    """


class SpanIFrame(TypedDict):
    id: str
    """
    Unique identifier for the span iframe
    """
    project_id: str
    """
    Unique identifier for the project that the span iframe belongs under
    """
    user_id: NotRequired[str]
    """
    Identifies the user who created the span iframe
    """
    created: NotRequired[str]
    """
    Date of span iframe creation
    """
    deleted_at: NotRequired[str]
    """
    Date of span iframe deletion, or null if the span iframe is still active
    """
    name: str
    """
    Name of the span iframe
    """
    description: NotRequired[str]
    """
    Textual description of the span iframe
    """
    url: str
    """
    URL to embed the project viewer in an iframe
    """
    post_message: NotRequired[bool]
    """
    Whether to post messages to the iframe containing the span's data. This is useful when you want to render more data than fits in the URL.
    """


class CreateSpanIFrame(TypedDict):
    project_id: str
    """
    Unique identifier for the project that the span iframe belongs under
    """
    name: str
    """
    Name of the span iframe
    """
    description: NotRequired[str]
    """
    Textual description of the span iframe
    """
    url: str
    """
    URL to embed the project viewer in an iframe
    """
    post_message: NotRequired[bool]
    """
    Whether to post messages to the iframe containing the span's data. This is useful when you want to render more data than fits in the URL.
    """


class PatchSpanIFrame(TypedDict):
    name: NotRequired[str]
    """
    Name of the span iframe
    """
    url: NotRequired[str]
    """
    URL to embed the project viewer in an iframe
    """
    post_message: NotRequired[bool]
    """
    Whether to post messages to the iframe containing the span's data. This is useful when you want to render more data than fits in the URL.
    """


class RuntimeContext(TypedDict):
    runtime: Literal["node", "python"]
    version: str


class Position(TypedDict):
    type: Literal["task"]


class Position1(TypedDict):
    type: Literal["scorer"]
    index: int


class Location(TypedDict):
    type: Literal["experiment"]
    eval_name: str
    position: Union[Position, Position1]


class Location1(TypedDict):
    type: Literal["function"]
    index: int


class CodeBundle(TypedDict):
    runtime_context: RuntimeContext
    location: Union[Location, Location1]
    bundle_id: str
    preview: NotRequired[str]
    """
    A preview of the code
    """


class FunctionData1(TypedDict):
    type: Literal["prompt"]


class Data(CodeBundle):
    type: Literal["bundle"]


class Data1(TypedDict):
    type: Literal["inline"]
    runtime_context: RuntimeContext
    code: str


class FunctionData2(TypedDict):
    type: Literal["code"]
    data: Union[Data, Data1]


class FunctionData3(TypedDict):
    type: Literal["global"]
    name: str


FunctionData = Union[FunctionData1, FunctionData2, FunctionData3]


class Origin4(TypedDict):
    object_type: AclObjectType
    object_id: str
    """
    Id of the object the function is originating from
    """
    internal: NotRequired[bool]
    """
    The function exists for internal purposes and should not be displayed in the list of functions.
    """


class FunctionSchema(TypedDict):
    parameters: NotRequired[Any]
    returns: NotRequired[Any]


class Function2(TypedDict):
    id: str
    """
    Unique identifier for the prompt
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the prompt (see the `version` parameter)
    """
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    log_id: Literal["p"]
    """
    A literal 'p' which identifies the object as a project prompt
    """
    org_id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    description: NotRequired[str]
    """
    Textual description of the prompt
    """
    created: NotRequired[str]
    """
    Date of prompt creation
    """
    prompt_data: NotRequired[PromptData]
    tags: NotRequired[Sequence[str]]
    """
    A list of tags for the prompt
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    User-controlled metadata about the prompt
    """
    function_type: NotRequired[Literal["llm", "scorer", "task", "tool"]]
    function_data: FunctionData
    origin: NotRequired[Origin4]
    function_schema: NotRequired[FunctionSchema]
    """
    JSON schema for the function's parameters and return type
    """


class CreateFunction(TypedDict):
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    description: NotRequired[str]
    """
    Textual description of the prompt
    """
    prompt_data: NotRequired[PromptData]
    tags: NotRequired[Sequence[str]]
    """
    A list of tags for the prompt
    """
    function_type: NotRequired[Literal["llm", "scorer", "task", "tool"]]
    function_data: FunctionData
    origin: NotRequired[Origin4]
    function_schema: NotRequired[FunctionSchema]
    """
    JSON schema for the function's parameters and return type
    """


class FunctionDataNullish1(TypedDict):
    type: Literal["prompt"]


class Data2(CodeBundle):
    type: Literal["bundle"]


class Data3(TypedDict):
    type: Literal["inline"]
    runtime_context: RuntimeContext
    code: str


class FunctionDataNullish2(TypedDict):
    type: Literal["code"]
    data: Union[Data2, Data3]


class FunctionDataNullish3(TypedDict):
    type: Literal["global"]
    name: str


FunctionDataNullish = Union[FunctionDataNullish1, FunctionDataNullish2, FunctionDataNullish3, Mapping[str, Any]]


class PatchFunction(TypedDict):
    name: NotRequired[str]
    """
    Name of the prompt
    """
    description: NotRequired[str]
    """
    Textual description of the prompt
    """
    prompt_data: NotRequired[PromptData]
    function_data: NotRequired[FunctionDataNullish]
    tags: NotRequired[Sequence[str]]
    """
    A list of tags for the prompt
    """


class RowIds(TypedDict):
    id: str
    """
    The id of the row
    """
    span_id: str
    """
    The span_id of the row
    """
    root_span_id: str
    """
    The root_span_id of the row
    """


class Parent(TypedDict):
    object_type: Literal["project_logs", "experiment"]
    object_id: str
    """
    The id of the container object you are logging to
    """
    row_ids: NotRequired[RowIds]
    """
    Identifiers for the row to to log a subspan under
    """
    propagated_event: NotRequired[Mapping[str, Any]]
    """
    Include these properties in every span created under this parent
    """


class InvokeApi(TypedDict):
    input: NotRequired[Any]
    """
    Argument to the function, which can be any JSON serializable value
    """
    messages: NotRequired[Sequence[ChatCompletionMessageParam]]
    """
    If the function is an LLM, additional messages to pass along to it
    """
    parent: NotRequired[Union[Parent, str]]
    """
    Options for tracing the function call
    """
    stream: NotRequired[bool]
    """
    Whether to stream the response. If true, results will be returned in the Braintrust SSE format.
    """
    mode: NotRequired[Literal["auto", "parallel"]]
    """
    The mode format of the returned value (defaults to 'auto')
    """
    version: NotRequired[str]
    """
    The version of the function
    """


class ViewDataSearch(TypedDict):
    filter: NotRequired[Sequence[Any]]
    tag: NotRequired[Sequence[Any]]
    match: NotRequired[Sequence[Any]]
    sort: NotRequired[Sequence[Any]]


class ViewData(TypedDict):
    search: NotRequired[ViewDataSearch]


class ViewOptions(TypedDict):
    columnVisibility: NotRequired[Mapping[str, bool]]
    columnOrder: NotRequired[Sequence[str]]
    columnSizing: NotRequired[Mapping[str, float]]


class View(TypedDict):
    id: str
    """
    Unique identifier for the view
    """
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """
    view_type: Literal[
        "projects", "logs", "experiments", "datasets", "prompts", "playgrounds", "experiment", "dataset"
    ]
    """
    Type of table that the view corresponds to.
    """
    name: str
    """
    Name of the view
    """
    created: NotRequired[str]
    """
    Date of view creation
    """
    view_data: NotRequired[ViewData]
    options: NotRequired[ViewOptions]
    user_id: NotRequired[str]
    """
    Identifies the user who created the view
    """
    deleted_at: NotRequired[str]
    """
    Date of role deletion, or null if the role is still active
    """


class CreateView(TypedDict):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """
    view_type: Literal[
        "projects", "logs", "experiments", "datasets", "prompts", "playgrounds", "experiment", "dataset"
    ]
    """
    Type of table that the view corresponds to.
    """
    name: str
    """
    Name of the view
    """
    view_data: NotRequired[ViewData]
    options: NotRequired[ViewOptions]
    user_id: NotRequired[str]
    """
    Identifies the user who created the view
    """
    deleted_at: NotRequired[str]
    """
    Date of role deletion, or null if the role is still active
    """


class PatchView(TypedDict):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """
    view_type: NotRequired[
        Literal["projects", "logs", "experiments", "datasets", "prompts", "playgrounds", "experiment", "dataset"]
    ]
    """
    Type of table that the view corresponds to.
    """
    name: NotRequired[str]
    """
    Name of the view
    """
    view_data: NotRequired[ViewData]
    options: NotRequired[ViewOptions]
    user_id: NotRequired[str]
    """
    Identifies the user who created the view
    """


class DeleteView(TypedDict):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """


class Organization(TypedDict):
    id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the organization
    """
    api_url: NotRequired[str]
    is_universal_api: NotRequired[bool]
    proxy_url: NotRequired[str]
    realtime_url: NotRequired[str]
    created: NotRequired[str]
    """
    Date of organization creation
    """


class PatchOrganization(TypedDict):
    name: NotRequired[str]
    """
    Name of the organization
    """
    api_url: NotRequired[str]
    is_universal_api: NotRequired[bool]
    proxy_url: NotRequired[str]
    realtime_url: NotRequired[str]


class PatchOrganizationMembersOutput(TypedDict):
    status: Literal["success"]
    send_email_error: NotRequired[str]
    """
    If invite emails failed to send for some reason, the patch operation will still complete, but we will return an error message here
    """


class InviteUsers(TypedDict):
    ids: NotRequired[Sequence[str]]
    """
    Ids of existing users to invite
    """
    emails: NotRequired[Sequence[str]]
    """
    Emails of users to invite
    """
    send_invite_emails: NotRequired[bool]
    """
    If true, send invite emails to the users who wore actually added
    """
    group_ids: NotRequired[Sequence[str]]
    """
    Optional list of group ids to add newly-invited users to.
    """
    group_names: NotRequired[Sequence[str]]
    """
    Optional list of group names to add newly-invited users to.
    """
    group_id: NotRequired[str]
    """
    Singular form of group_ids
    """
    group_name: NotRequired[str]
    """
    Singular form of group_names
    """


class RemoveUsers(TypedDict):
    ids: NotRequired[Sequence[str]]
    """
    Ids of users to remove
    """
    emails: NotRequired[Sequence[str]]
    """
    Emails of users to remove
    """


class PatchOrganizationMembers(TypedDict):
    invite_users: NotRequired[InviteUsers]
    """
    Users to invite to the organization
    """
    remove_users: NotRequired[RemoveUsers]
    """
    Users to remove from the organization
    """
    org_name: NotRequired[str]
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, or in case you want to explicitly assert the organization you are modifying, you may specify the name of the organization.
    """
    org_id: NotRequired[str]
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, or in case you want to explicitly assert the organization you are modifying, you may specify the id of the organization.
    """


class CreateApiKeyOutput(TypedDict):
    id: str
    """
    Unique identifier for the api key
    """
    created: NotRequired[str]
    """
    Date of api key creation
    """
    name: str
    """
    Name of the api key
    """
    preview_name: str
    user_id: NotRequired[str]
    """
    Unique identifier for the user
    """
    org_id: NotRequired[str]
    """
    Unique identifier for the organization
    """
    key: str
    """
    The raw API key. It will only be exposed this one time
    """


class ApiKey(TypedDict):
    id: str
    """
    Unique identifier for the api key
    """
    created: NotRequired[str]
    """
    Date of api key creation
    """
    name: str
    """
    Name of the api key
    """
    preview_name: str
    user_id: NotRequired[str]
    """
    Unique identifier for the user
    """
    org_id: NotRequired[str]
    """
    Unique identifier for the organization
    """


class AISecret(TypedDict):
    id: str
    """
    Unique identifier for the AI secret
    """
    created: NotRequired[str]
    """
    Date of AI secret creation
    """
    org_id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the AI secret
    """
    type: NotRequired[str]
    metadata: NotRequired[Mapping[str, Any]]
    preview_secret: NotRequired[str]


class CreateAISecret(TypedDict):
    name: str
    """
    Name of the AI secret
    """
    type: NotRequired[str]
    metadata: NotRequired[Mapping[str, Any]]
    secret: NotRequired[str]
    """
    Secret value. If omitted in a PUT request, the existing secret value will be left intact, not replaced with null.
    """
    org_name: NotRequired[str]
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the AI Secret belongs in.
    """


class DeleteAISecret(TypedDict):
    name: str
    """
    Name of the AI secret
    """
    org_name: NotRequired[str]
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the AI Secret belongs in.
    """


class PatchAISecret(TypedDict):
    name: NotRequired[str]
    """
    Name of the AI secret
    """
    type: NotRequired[str]
    metadata: NotRequired[Mapping[str, Any]]
    secret: NotRequired[str]


class EnvVar(TypedDict):
    id: str
    """
    Unique identifier for the environment variable
    """
    object_type: Literal["organization", "project", "function"]
    """
    The type of the object the environment variable is scoped for
    """
    object_id: str
    """
    The id of the object the environment variable is scoped for
    """
    name: str
    """
    The name of the environment variable
    """
    created: NotRequired[str]
    """
    Date of environment variable creation
    """
    used: NotRequired[str]
    """
    Date the environment variable was last used
    """


class CrossObjectInsertResponse(TypedDict):
    experiment: NotRequired[Mapping[str, InsertEventsResponse]]
    """
    A mapping from experiment id to row ids for inserted `events`
    """
    dataset: NotRequired[Mapping[str, InsertEventsResponse]]
    """
    A mapping from dataset id to row ids for inserted `events`
    """
    project_logs: NotRequired[Mapping[str, InsertEventsResponse]]
    """
    A mapping from project id to row ids for inserted `events`
    """


class Experiment1(TypedDict):
    events: NotRequired[Sequence[InsertExperimentEvent]]
    """
    A list of experiment events to insert
    """
    feedback: NotRequired[Sequence[FeedbackExperimentItem]]
    """
    A list of experiment feedback items
    """


class Dataset1(TypedDict):
    events: NotRequired[Sequence[InsertDatasetEvent]]
    """
    A list of dataset events to insert
    """
    feedback: NotRequired[Sequence[FeedbackDatasetItem]]
    """
    A list of dataset feedback items
    """


class ProjectLogs(TypedDict):
    events: NotRequired[Sequence[InsertProjectLogsEvent]]
    """
    A list of project logs events to insert
    """
    feedback: NotRequired[Sequence[FeedbackProjectLogsItem]]
    """
    A list of project logs feedback items
    """


class CrossObjectInsertRequest(TypedDict):
    experiment: NotRequired[Mapping[str, Experiment1]]
    """
    A mapping from experiment id to a set of log events and feedback items to insert
    """
    dataset: NotRequired[Mapping[str, Dataset1]]
    """
    A mapping from dataset id to a set of log events and feedback items to insert
    """
    project_logs: NotRequired[Mapping[str, ProjectLogs]]
    """
    A mapping from project id to a set of log events and feedback items to insert
    """


class FunctionId1(TypedDict):
    function_id: str
    """
    The ID of the function
    """
    version: NotRequired[str]
    """
    The version of the function
    """


class FunctionId2(TypedDict):
    project_name: str
    """
    The name of the project containing the function
    """
    slug: str
    """
    The slug of the function
    """
    version: NotRequired[str]
    """
    The version of the function
    """


class FunctionId3(TypedDict):
    global_function: str
    """
    The name of the global function. Currently, the global namespace includes the functions in autoevals
    """


class FunctionId4(TypedDict):
    prompt_session_id: str
    """
    The ID of the prompt session
    """
    prompt_session_function_id: str
    """
    The ID of the function in the prompt session
    """
    version: NotRequired[str]
    """
    The version of the function
    """


class InlineContext(TypedDict):
    runtime: Literal["node", "python"]
    version: str


class FunctionId5(TypedDict):
    inline_context: InlineContext
    code: str
    """
    The inline code to execute
    """
    name: NotRequired[str]
    """
    The name of the inline code function
    """


class FunctionId6(TypedDict):
    inline_prompt: PromptData
    name: NotRequired[str]
    """
    The name of the inline prompt
    """


FunctionId = Union[FunctionId1, FunctionId2, FunctionId3, FunctionId4, FunctionId5, FunctionId6]


class Data4(TypedDict):
    dataset_id: str


class Data5(TypedDict):
    project_name: str
    dataset_name: str


class GitMetadataSettings(TypedDict):
    collect: Literal["all", "none", "some"]
    fields: NotRequired[
        Sequence[
            Literal[
                "commit",
                "branch",
                "tag",
                "dirty",
                "author_name",
                "author_email",
                "commit_message",
                "commit_time",
                "git_diff",
            ]
        ]
    ]


class RunEval(TypedDict):
    project_id: str
    """
    Unique identifier for the project to run the eval in
    """
    data: Union[Data4, Data5]
    """
    The dataset to use
    """
    task: FunctionId
    scores: Sequence[FunctionId]
    """
    The functions to score the eval on
    """
    experiment_name: NotRequired[str]
    """
    An optional name for the experiment created by this eval. If it conflicts with an existing experiment, it will be suffixed with a unique identifier.
    """
    metadata: NotRequired[Mapping[str, Any]]
    """
    Optional experiment-level metadata to store about the evaluation. You can later use this to slice & dice across experiments.
    """
    stream: NotRequired[bool]
    """
    Whether to stream the results of the eval. If true, the request will return two events: one to indicate the experiment has started, and another upon completion. If false, the request will return the evaluation's summary upon completion.
    """
    trial_count: NotRequired[float]
    """
    The number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.
    """
    is_public: NotRequired[bool]
    """
    Whether the experiment should be public. Defaults to false.
    """
    timeout: NotRequired[float]
    """
    The maximum duration, in milliseconds, to run the evaluation. Defaults to undefined, in which case there is no timeout.
    """
    max_concurrency: NotRequired[float]
    """
    The maximum number of tasks/scorers that will be run concurrently. Defaults to undefined, in which case there is no max concurrency.
    """
    base_experiment_name: NotRequired[str]
    """
    An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.
    """
    base_experiment_id: NotRequired[str]
    """
    An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this experiment.
    """
    git_metadata_settings: NotRequired[GitMetadataSettings]
    """
    Optional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.
    """
    repo_info: NotRequired[RepoInfo]
