"""Auto-generated file -- do not modify."""

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Union

from braintrust_core.serializable_data_class import SerializableDataClass

ProjectIdParam = str


ExperimentIdParam = str


DatasetIdParam = str


PromptIdParam = str


PromptSessionIdParam = str


RoleIdParam = str


GroupIdParam = str


AclIdParam = str


UserIdParam = str


ProjectScoreIdParam = str


ProjectTagIdParam = str


SpanIframeIdParam = str


FunctionIdParam = str


ViewIdParam = str


OrganizationIdParam = str


ApiKeyIdParam = str


AiSecretIdParam = str


EnvVarIdParam = str


ProjectIdQuery = str


ProjectName = str


ExperimentName = str


DatasetName = str


PromptName = str


PromptSessionName = str


RoleName = str


GroupName = str


ProjectScoreName = str


ProjectTagName = str


SpanIframeName = str


FunctionName = str


ViewName = str


ApiKeyName = str


AiSecretName = str


EnvVarName = str


OrgName = str


Ids = Union[str, List[str]]


AppLimitParam = Optional[int]


FetchLimitParam = Optional[int]


StartingAfter = str


EndingBefore = str


MaxXactId = str


MaxRootSpanId = str


Version = str


PromptVersion = str


SummarizeScores = Optional[bool]


ComparisonExperimentId = str


SummarizeData = Optional[bool]


Slug = str


class ViewTypeEnum(Enum):
    PROJECTS = "projects"
    LOGS = "logs"
    EXPERIMENTS = "experiments"
    DATASETS = "datasets"
    PROMPTS = "prompts"
    PLAYGROUNDS = "playgrounds"
    EXPERIMENT = "experiment"
    DATASET = "dataset"


ViewType = Optional[ViewTypeEnum]


UserGivenName = Union[str, List[str]]


UserFamilyName = Union[str, List[str]]


UserEmail = Union[str, List[str]]


class AclObjectType(Enum):
    ORGANIZATION = "organization"
    PROJECT = "project"
    EXPERIMENT = "experiment"
    DATASET = "dataset"
    PROMPT = "prompt"
    PROMPT_SESSION = "prompt_session"
    GROUP = "group"
    ROLE = "role"
    ORG_MEMBER = "org_member"
    PROJECT_LOG = "project_log"
    ORG_PROJECT = "org_project"


AclObjectId = str


class ProjectScoreType(Enum):
    SLIDER = "slider"
    CATEGORICAL = "categorical"
    WEIGHTED = "weighted"
    MINIMUM = "minimum"
    MAXIMUM = "maximum"
    ONLINE = "online"


AISecretType = Union[str, List[str]]


class EnvVarObjectType(Enum):
    ORGANIZATION = "organization"
    PROJECT = "project"
    FUNCTION = "function"


EnvVarObjectId = str


@dataclass
class ProjectSettings(SerializableDataClass):
    comparison_key: Optional[str] = None
    """
    The key used to join two experiments (defaults to `input`).
    """


@dataclass
class Project(SerializableDataClass):
    id: str
    """
    Unique identifier for the project
    """
    org_id: str
    """
    Unique id for the organization that the project belongs under
    """
    name: str
    """
    Name of the project
    """
    created: Optional[str] = None
    """
    Date of project creation
    """
    deleted_at: Optional[str] = None
    """
    Date of project deletion, or null if the project is still active
    """
    user_id: Optional[str] = None
    """
    Identifies the user who created the project
    """
    settings: Optional[ProjectSettings] = None


@dataclass
class CreateProject(SerializableDataClass):
    name: str
    """
    Name of the project
    """
    org_name: Optional[str] = None
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the project belongs in.
    """


@dataclass
class PatchProject(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the project
    """
    settings: Optional[ProjectSettings] = None


@dataclass
class InsertEventsResponse(SerializableDataClass):
    row_ids: List[str]
    """
    The ids of all rows that were inserted, aligning one-to-one with the rows provided as input
    """


@dataclass
class Metrics(SerializableDataClass):
    start: Optional[float] = None
    """
    A unix timestamp recording when the section of code which produced the project logs event started
    """
    end: Optional[float] = None
    """
    A unix timestamp recording when the section of code which produced the project logs event finished
    """
    prompt_tokens: Optional[int] = None
    """
    The number of tokens in the prompt used to generate the project logs event (only set if this is an LLM span)
    """
    completion_tokens: Optional[int] = None
    """
    The number of tokens in the completion generated by the model (only set if this is an LLM span)
    """
    tokens: Optional[int] = None
    """
    The total number of tokens in the input and output of the project logs event.
    """


@dataclass
class Context(SerializableDataClass):
    caller_functionname: Optional[str] = None
    """
    The function in code which created the project logs event
    """
    caller_filename: Optional[str] = None
    """
    Name of the file in code where the project logs event was created
    """
    caller_lineno: Optional[int] = None
    """
    Line of code where the project logs event was created
    """


class TypeEnum(Enum):
    LLM = "llm"
    SCORE = "score"
    FUNCTION = "function"
    EVAL = "eval"
    TASK = "task"
    TOOL = "tool"


Type = Optional[TypeEnum]


@dataclass
class SpanAttributes(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the span, for display purposes only
    """
    type: Optional[Type] = None
    """
    Type of the span, for display purposes only
    """


class FieldIsMerge(Enum):
    BOOLEAN_FALSE = False
    BOOLEAN_NONE = None


@dataclass
class InsertProjectLogsEventReplace(SerializableDataClass):
    input: Optional[Any] = None
    """
    The arguments that uniquely define a user input (an arbitrary, JSON serializable object).
    """
    output: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
    """
    error: Optional[Any] = None
    """
    The error that occurred, if any.
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    metrics: Optional[Metrics] = None
    """
    Metrics are numerical measurements tracking the execution of the code that produced the project logs event. Use "start" and "end" to track the time span over which the project logs event was produced
    """
    context: Optional[Context] = None
    """
    Context is additional information about the code that produced the project logs event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the project logs event
    """
    span_attributes: Optional[SpanAttributes] = None
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: Optional[str] = None
    """
    A unique identifier for the project logs event. If you don't provide one, BrainTrust will generate one for you
    """
    created: Optional[str] = None
    """
    The timestamp the project logs event was created
    """
    _object_delete: Optional[bool] = None
    """
    Pass `_object_delete=true` to mark the project logs event deleted. Deleted events will not show up in subsequent fetches for this project logs
    """
    _is_merge: Optional[FieldIsMerge] = None
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _parent_id: Optional[str] = None
    """
    Use the `_parent_id` field to create this row as a subspan of an existing row. It cannot be specified alongside `_is_merge=true`. Tracking hierarchical relationships are important for tracing (see the [guide](https://www.braintrust.dev/docs/guides/tracing) for full details).

    For example, say we have logged a row `{"id": "abc", "input": "foo", "output": "bar", "expected": "boo", "scores": {"correctness": 0.33}}`. We can create a sub-span of the parent row by logging `{"_parent_id": "abc", "id": "llm_call", "input": {"prompt": "What comes after foo?"}, "output": "bar", "metrics": {"tokens": 1}}`. In the webapp, only the root span row `"abc"` will show up in the summary view. You can view the full trace hierarchy (in this case, the `"llm_call"` row) by clicking on the "abc" row.
    """


class Type1Enum(Enum):
    LLM = "llm"
    SCORE = "score"
    FUNCTION = "function"
    EVAL = "eval"
    TASK = "task"
    TOOL = "tool"


Type1 = Optional[Type1Enum]


@dataclass
class SpanAttributes1(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the span, for display purposes only
    """
    type: Optional[Type1] = None
    """
    Type of the span, for display purposes only
    """


class _IsMerge(Enum):
    BOOLEAN_TRUE = True


@dataclass
class InsertProjectLogsEventMerge(SerializableDataClass):
    _is_merge: _IsMerge
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    input: Optional[Any] = None
    """
    The arguments that uniquely define a user input (an arbitrary, JSON serializable object).
    """
    output: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
    """
    error: Optional[Any] = None
    """
    The error that occurred, if any.
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    metrics: Optional[Metrics] = None
    """
    Metrics are numerical measurements tracking the execution of the code that produced the project logs event. Use "start" and "end" to track the time span over which the project logs event was produced
    """
    context: Optional[Context] = None
    """
    Context is additional information about the code that produced the project logs event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the project logs event
    """
    span_attributes: Optional[SpanAttributes1] = None
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: Optional[str] = None
    """
    A unique identifier for the project logs event. If you don't provide one, BrainTrust will generate one for you
    """
    created: Optional[str] = None
    """
    The timestamp the project logs event was created
    """
    _object_delete: Optional[bool] = None
    """
    Pass `_object_delete=true` to mark the project logs event deleted. Deleted events will not show up in subsequent fetches for this project logs
    """
    _merge_paths: Optional[List[List[str]]] = None
    """
    The `_merge_paths` field allows controlling the depth of the merge. It can only be specified alongside `_is_merge=true`. `_merge_paths` is a list of paths, where each path is a list of field names. The deep merge will not descend below any of the specified merge paths.

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": {"b": 10}, "c": {"d": 20}}, "output": {"a": 20}}`. If we merge a new row as `{"_is_merge": true, "_merge_paths": [["input", "a"], ["output"]], "input": {"a": {"q": 30}, "c": {"e": 30}, "bar": "baz"}, "output": {"d": 40}}`, the new row will be `{"id": "foo": "input": {"a": {"q": 30}, "c": {"d": 20, "e": 30}, "bar": "baz"}, "output": {"d": 40}}`. In this case, due to the merge paths, we have replaced `input.a` and `output`, but have still deep-merged `input` and `input.c`.
    """


InsertProjectLogsEvent = Union[InsertProjectLogsEventReplace, InsertProjectLogsEventMerge]


@dataclass
class InsertProjectLogsEventRequest(SerializableDataClass):
    events: List[InsertProjectLogsEvent]
    """
    A list of project logs events to insert
    """


class LogId(Enum):
    G = "g"


class Type2Enum(Enum):
    LLM = "llm"
    SCORE = "score"
    FUNCTION = "function"
    EVAL = "eval"
    TASK = "task"
    TOOL = "tool"


Type2 = Optional[Type2Enum]


@dataclass
class SpanAttributes2(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the span, for display purposes only
    """
    type: Optional[Type2] = None
    """
    Type of the span, for display purposes only
    """


class ObjectType(Enum):
    EXPERIMENT = "experiment"
    DATASET = "dataset"
    PROMPT = "prompt"
    FUNCTION = "function"
    PROMPT_SESSION = "prompt_session"


class ObjectType1(Enum):
    PROJECT_LOGS = "project_logs"


@dataclass
class Origin(SerializableDataClass):
    object_type: Union[ObjectType, ObjectType1]
    """
    Type of the object the event is originating from.
    """
    object_id: str
    """
    ID of the object the event is originating from.
    """
    id: str
    """
    ID of the original event.
    """
    _xact_id: str
    """
    Transaction ID of the original event.
    """


@dataclass
class ProjectLogsEvent(SerializableDataClass):
    id: str
    """
    A unique identifier for the project logs event. If you don't provide one, BrainTrust will generate one for you
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the project logs (see the `version` parameter)
    """
    created: str
    """
    The timestamp the project logs event was created
    """
    org_id: str
    """
    Unique id for the organization that the project belongs under
    """
    project_id: str
    """
    Unique identifier for the project
    """
    log_id: LogId
    """
    A literal 'g' which identifies the log as a project log
    """
    span_id: str
    """
    A unique identifier used to link different project logs events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this project logs event belongs to
    """
    input: Optional[Any] = None
    """
    The arguments that uniquely define a user input (an arbitrary, JSON serializable object).
    """
    output: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
    """
    error: Optional[Any] = None
    """
    The error that occurred, if any.
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    metrics: Optional[Metrics] = None
    """
    Metrics are numerical measurements tracking the execution of the code that produced the project logs event. Use "start" and "end" to track the time span over which the project logs event was produced
    """
    context: Optional[Context] = None
    """
    Context is additional information about the code that produced the project logs event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the project logs event
    """
    span_parents: Optional[List[str]] = None
    """
    An array of the parent `span_ids` of this project logs event. This should be empty for the root span of a trace, and should most often contain just one parent element for subspans
    """
    is_root: Optional[bool] = None
    """
    Whether this span is a root span
    """
    span_attributes: Optional[SpanAttributes2] = None
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    origin: Optional[Origin] = None
    """
    Indicates the event was copied from another object.
    """


@dataclass
class FetchProjectLogsEventsResponse(SerializableDataClass):
    events: List[ProjectLogsEvent]
    """
    A list of fetched events
    """
    cursor: Optional[str] = None
    """
    Pagination cursor

    Pass this string directly as the `cursor` param to your next fetch request to get the next page of results. Not provided if the returned result set is empty.
    """


FetchLimit = Optional[int]


FetchPaginationCursor = Optional[str]


class Type3(Enum):
    PATH_LOOKUP = "path_lookup"


@dataclass
class PathLookupFilter(SerializableDataClass):
    type: Type3
    """
    Denotes the type of filter as a path-lookup filter
    """
    path: List[str]
    """
    List of fields describing the path to the value to be checked against. For instance, if you wish to filter on the value of `c` in `{"input": {"a": {"b": {"c": "hello"}}}}`, pass `path=["input", "a", "b", "c"]`
    """
    value: Optional[Any] = None
    """
    The value to compare equality-wise against the event value at the specified `path`. The value must be a "primitive", that is, any JSON-serializable object except for objects and arrays. For instance, if you wish to filter on the value of "input.a.b.c" in the object `{"input": {"a": {"b": {"c": "hello"}}}}`, pass `value="hello"`
    """


FetchEventsFilters = Optional[List[PathLookupFilter]]


@dataclass
class FetchEventsRequest(SerializableDataClass):
    limit: Optional[FetchLimit] = None
    cursor: Optional[FetchPaginationCursor] = None
    max_xact_id: Optional[MaxXactId] = None
    max_root_span_id: Optional[MaxRootSpanId] = None
    filters: Optional[FetchEventsFilters] = None
    version: Optional[Version] = None


class Status(Enum):
    SUCCESS = "success"


@dataclass
class FeedbackResponseSchema(SerializableDataClass):
    status: Status


class SourceEnum(Enum):
    APP = "app"
    API = "api"
    EXTERNAL = "external"


Source = Optional[SourceEnum]


@dataclass
class FeedbackProjectLogsItem(SerializableDataClass):
    id: str
    """
    The id of the project logs event to log feedback for. This is the row `id` returned by `POST /v1/project_logs/{project_id}/insert`
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the project logs event
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not
    """
    comment: Optional[str] = None
    """
    An optional comment string to log about the project logs event
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
    """
    source: Optional[Source] = None
    """
    The source of the feedback. Must be one of "external" (default), "app", or "api"
    """


@dataclass
class FeedbackProjectLogsEventRequest(SerializableDataClass):
    feedback: List[FeedbackProjectLogsItem]
    """
    A list of project logs feedback items
    """


@dataclass
class RepoInfo(SerializableDataClass):
    commit: Optional[str] = None
    """
    SHA of most recent commit
    """
    branch: Optional[str] = None
    """
    Name of the branch the most recent commit belongs to
    """
    tag: Optional[str] = None
    """
    Name of the tag on the most recent commit
    """
    dirty: Optional[bool] = None
    """
    Whether or not the repo had uncommitted changes when snapshotted
    """
    author_name: Optional[str] = None
    """
    Name of the author of the most recent commit
    """
    author_email: Optional[str] = None
    """
    Email of the author of the most recent commit
    """
    commit_message: Optional[str] = None
    """
    Most recent commit message
    """
    commit_time: Optional[str] = None
    """
    Time of the most recent commit
    """
    git_diff: Optional[str] = None
    """
    If the repo was dirty when run, this includes the diff between the current state of the repo and the most recent commit.
    """


@dataclass
class Experiment(SerializableDataClass):
    id: str
    """
    Unique identifier for the experiment
    """
    project_id: str
    """
    Unique identifier for the project that the experiment belongs under
    """
    name: str
    """
    Name of the experiment. Within a project, experiment names are unique
    """
    public: bool
    """
    Whether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization
    """
    description: Optional[str] = None
    """
    Textual description of the experiment
    """
    created: Optional[str] = None
    """
    Date of experiment creation
    """
    repo_info: Optional[RepoInfo] = None
    commit: Optional[str] = None
    """
    Commit, taken directly from `repo_info.commit`
    """
    base_exp_id: Optional[str] = None
    """
    Id of default base experiment to compare against when viewing this experiment
    """
    deleted_at: Optional[str] = None
    """
    Date of experiment deletion, or null if the experiment is still active
    """
    dataset_id: Optional[str] = None
    """
    Identifier of the linked dataset, or null if the experiment is not linked to a dataset
    """
    dataset_version: Optional[str] = None
    """
    Version number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.
    """
    user_id: Optional[str] = None
    """
    Identifies the user who created the experiment
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the experiment
    """


@dataclass
class CreateExperiment(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project that the experiment belongs under
    """
    name: Optional[str] = None
    """
    Name of the experiment. Within a project, experiment names are unique
    """
    description: Optional[str] = None
    """
    Textual description of the experiment
    """
    repo_info: Optional[RepoInfo] = None
    base_exp_id: Optional[str] = None
    """
    Id of default base experiment to compare against when viewing this experiment
    """
    dataset_id: Optional[str] = None
    """
    Identifier of the linked dataset, or null if the experiment is not linked to a dataset
    """
    dataset_version: Optional[str] = None
    """
    Version number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.
    """
    public: Optional[bool] = None
    """
    Whether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the experiment
    """
    ensure_new: Optional[bool] = None
    """
    Normally, creating an experiment with the same name as an existing experiment will return the existing one un-modified. But if `ensure_new` is true, registration will generate a new experiment with a unique name in case of a conflict.
    """


@dataclass
class PatchExperiment(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the experiment. Within a project, experiment names are unique
    """
    description: Optional[str] = None
    """
    Textual description of the experiment
    """
    repo_info: Optional[RepoInfo] = None
    base_exp_id: Optional[str] = None
    """
    Id of default base experiment to compare against when viewing this experiment
    """
    dataset_id: Optional[str] = None
    """
    Identifier of the linked dataset, or null if the experiment is not linked to a dataset
    """
    dataset_version: Optional[str] = None
    """
    Version number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.
    """
    public: Optional[bool] = None
    """
    Whether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the experiment
    """


@dataclass
class Metrics3(SerializableDataClass):
    start: Optional[float] = None
    """
    A unix timestamp recording when the section of code which produced the experiment event started
    """
    end: Optional[float] = None
    """
    A unix timestamp recording when the section of code which produced the experiment event finished
    """
    prompt_tokens: Optional[int] = None
    """
    The number of tokens in the prompt used to generate the experiment event (only set if this is an LLM span)
    """
    completion_tokens: Optional[int] = None
    """
    The number of tokens in the completion generated by the model (only set if this is an LLM span)
    """
    tokens: Optional[int] = None
    """
    The total number of tokens in the input and output of the experiment event.
    """


@dataclass
class Context3(SerializableDataClass):
    caller_functionname: Optional[str] = None
    """
    The function in code which created the experiment event
    """
    caller_filename: Optional[str] = None
    """
    Name of the file in code where the experiment event was created
    """
    caller_lineno: Optional[int] = None
    """
    Line of code where the experiment event was created
    """


class Type4Enum(Enum):
    LLM = "llm"
    SCORE = "score"
    FUNCTION = "function"
    EVAL = "eval"
    TASK = "task"
    TOOL = "tool"


Type4 = Optional[Type4Enum]


@dataclass
class SpanAttributes3(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the span, for display purposes only
    """
    type: Optional[Type4] = None
    """
    Type of the span, for display purposes only
    """


class FieldIsMerge2(Enum):
    BOOLEAN_FALSE = False
    BOOLEAN_NONE = None


@dataclass
class InsertExperimentEventReplace(SerializableDataClass):
    input: Optional[Any] = None
    """
    The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical
    """
    output: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models
    """
    error: Optional[Any] = None
    """
    The error that occurred, if any.
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    metrics: Optional[Metrics3] = None
    """
    Metrics are numerical measurements tracking the execution of the code that produced the experiment event. Use "start" and "end" to track the time span over which the experiment event was produced
    """
    context: Optional[Context3] = None
    """
    Context is additional information about the code that produced the experiment event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the experiment event
    """
    span_attributes: Optional[SpanAttributes3] = None
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: Optional[str] = None
    """
    A unique identifier for the experiment event. If you don't provide one, BrainTrust will generate one for you
    """
    dataset_record_id: Optional[str] = None
    """
    If the experiment is associated to a dataset, this is the event-level dataset id this experiment event is tied to
    """
    created: Optional[str] = None
    """
    The timestamp the experiment event was created
    """
    _object_delete: Optional[bool] = None
    """
    Pass `_object_delete=true` to mark the experiment event deleted. Deleted events will not show up in subsequent fetches for this experiment
    """
    _is_merge: Optional[FieldIsMerge2] = None
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _parent_id: Optional[str] = None
    """
    Use the `_parent_id` field to create this row as a subspan of an existing row. It cannot be specified alongside `_is_merge=true`. Tracking hierarchical relationships are important for tracing (see the [guide](https://www.braintrust.dev/docs/guides/tracing) for full details).

    For example, say we have logged a row `{"id": "abc", "input": "foo", "output": "bar", "expected": "boo", "scores": {"correctness": 0.33}}`. We can create a sub-span of the parent row by logging `{"_parent_id": "abc", "id": "llm_call", "input": {"prompt": "What comes after foo?"}, "output": "bar", "metrics": {"tokens": 1}}`. In the webapp, only the root span row `"abc"` will show up in the summary view. You can view the full trace hierarchy (in this case, the `"llm_call"` row) by clicking on the "abc" row.
    """


class Type5Enum(Enum):
    LLM = "llm"
    SCORE = "score"
    FUNCTION = "function"
    EVAL = "eval"
    TASK = "task"
    TOOL = "tool"


Type5 = Optional[Type5Enum]


@dataclass
class SpanAttributes4(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the span, for display purposes only
    """
    type: Optional[Type5] = None
    """
    Type of the span, for display purposes only
    """


class FieldIsMerge3(Enum):
    BOOLEAN_TRUE = True


@dataclass
class InsertExperimentEventMerge(SerializableDataClass):
    _is_merge: FieldIsMerge3
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    input: Optional[Any] = None
    """
    The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical
    """
    output: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models
    """
    error: Optional[Any] = None
    """
    The error that occurred, if any.
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    metrics: Optional[Metrics3] = None
    """
    Metrics are numerical measurements tracking the execution of the code that produced the experiment event. Use "start" and "end" to track the time span over which the experiment event was produced
    """
    context: Optional[Context3] = None
    """
    Context is additional information about the code that produced the experiment event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the experiment event
    """
    span_attributes: Optional[SpanAttributes4] = None
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    id: Optional[str] = None
    """
    A unique identifier for the experiment event. If you don't provide one, BrainTrust will generate one for you
    """
    dataset_record_id: Optional[str] = None
    """
    If the experiment is associated to a dataset, this is the event-level dataset id this experiment event is tied to
    """
    created: Optional[str] = None
    """
    The timestamp the experiment event was created
    """
    _object_delete: Optional[bool] = None
    """
    Pass `_object_delete=true` to mark the experiment event deleted. Deleted events will not show up in subsequent fetches for this experiment
    """
    _merge_paths: Optional[List[List[str]]] = None
    """
    The `_merge_paths` field allows controlling the depth of the merge. It can only be specified alongside `_is_merge=true`. `_merge_paths` is a list of paths, where each path is a list of field names. The deep merge will not descend below any of the specified merge paths.

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": {"b": 10}, "c": {"d": 20}}, "output": {"a": 20}}`. If we merge a new row as `{"_is_merge": true, "_merge_paths": [["input", "a"], ["output"]], "input": {"a": {"q": 30}, "c": {"e": 30}, "bar": "baz"}, "output": {"d": 40}}`, the new row will be `{"id": "foo": "input": {"a": {"q": 30}, "c": {"d": 20, "e": 30}, "bar": "baz"}, "output": {"d": 40}}`. In this case, due to the merge paths, we have replaced `input.a` and `output`, but have still deep-merged `input` and `input.c`.
    """


InsertExperimentEvent = Union[InsertExperimentEventReplace, InsertExperimentEventMerge]


@dataclass
class InsertExperimentEventRequest(SerializableDataClass):
    events: List[InsertExperimentEvent]
    """
    A list of experiment events to insert
    """


class Type6Enum(Enum):
    LLM = "llm"
    SCORE = "score"
    FUNCTION = "function"
    EVAL = "eval"
    TASK = "task"
    TOOL = "tool"


Type6 = Optional[Type6Enum]


@dataclass
class SpanAttributes5(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the span, for display purposes only
    """
    type: Optional[Type6] = None
    """
    Type of the span, for display purposes only
    """


class ObjectType2(Enum):
    EXPERIMENT = "experiment"
    DATASET = "dataset"
    PROMPT = "prompt"
    FUNCTION = "function"
    PROMPT_SESSION = "prompt_session"


class ObjectType3(Enum):
    PROJECT_LOGS = "project_logs"


@dataclass
class Origin1(SerializableDataClass):
    object_type: Union[ObjectType2, ObjectType3]
    """
    Type of the object the event is originating from.
    """
    object_id: str
    """
    ID of the object the event is originating from.
    """
    id: str
    """
    ID of the original event.
    """
    _xact_id: str
    """
    Transaction ID of the original event.
    """


@dataclass
class ExperimentEvent(SerializableDataClass):
    id: str
    """
    A unique identifier for the experiment event. If you don't provide one, BrainTrust will generate one for you
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the experiment (see the `version` parameter)
    """
    created: str
    """
    The timestamp the experiment event was created
    """
    project_id: str
    """
    Unique identifier for the project that the experiment belongs under
    """
    experiment_id: str
    """
    Unique identifier for the experiment
    """
    span_id: str
    """
    A unique identifier used to link different experiment events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this experiment event belongs to
    """
    dataset_record_id: Optional[str] = None
    """
    If the experiment is associated to a dataset, this is the event-level dataset id this experiment event is tied to
    """
    input: Optional[Any] = None
    """
    The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical
    """
    output: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models
    """
    error: Optional[Any] = None
    """
    The error that occurred, if any.
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    metrics: Optional[Metrics3] = None
    """
    Metrics are numerical measurements tracking the execution of the code that produced the experiment event. Use "start" and "end" to track the time span over which the experiment event was produced
    """
    context: Optional[Context3] = None
    """
    Context is additional information about the code that produced the experiment event. It is essentially the textual counterpart to `metrics`. Use the `caller_*` attributes to track the location in code which produced the experiment event
    """
    span_parents: Optional[List[str]] = None
    """
    An array of the parent `span_ids` of this experiment event. This should be empty for the root span of a trace, and should most often contain just one parent element for subspans
    """
    span_attributes: Optional[SpanAttributes5] = None
    """
    Human-identifying attributes of the span, such as name, type, etc.
    """
    is_root: Optional[bool] = None
    """
    Whether this span is a root span
    """
    origin: Optional[Origin1] = None
    """
    Indicates the event was copied from another object.
    """


@dataclass
class FetchExperimentEventsResponse(SerializableDataClass):
    events: List[ExperimentEvent]
    """
    A list of fetched events
    """
    cursor: Optional[str] = None
    """
    Pagination cursor

    Pass this string directly as the `cursor` param to your next fetch request to get the next page of results. Not provided if the returned result set is empty.
    """


class Source1Enum(Enum):
    APP = "app"
    API = "api"
    EXTERNAL = "external"


Source1 = Optional[Source1Enum]


@dataclass
class FeedbackExperimentItem(SerializableDataClass):
    id: str
    """
    The id of the experiment event to log feedback for. This is the row `id` returned by `POST /v1/experiment/{experiment_id}/insert`
    """
    scores: Optional[Dict[str, float]] = None
    """
    A dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the experiment event
    """
    expected: Optional[Any] = None
    """
    The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not
    """
    comment: Optional[str] = None
    """
    An optional comment string to log about the experiment event
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
    """
    source: Optional[Source1] = None
    """
    The source of the feedback. Must be one of "external" (default), "app", or "api"
    """


@dataclass
class FeedbackExperimentEventRequest(SerializableDataClass):
    feedback: List[FeedbackExperimentItem]
    """
    A list of experiment feedback items
    """


@dataclass
class ScoreSummary(SerializableDataClass):
    name: str
    """
    Name of the score
    """
    score: float
    """
    Average score across all examples
    """
    improvements: int
    """
    Number of improvements in the score
    """
    regressions: int
    """
    Number of regressions in the score
    """
    diff: Optional[float] = None
    """
    Difference in score between the current and comparison experiment
    """


@dataclass
class MetricSummary(SerializableDataClass):
    name: str
    """
    Name of the metric
    """
    metric: float
    """
    Average metric across all examples
    """
    unit: str
    """
    Unit label for the metric
    """
    improvements: int
    """
    Number of improvements in the metric
    """
    regressions: int
    """
    Number of regressions in the metric
    """
    diff: Optional[float] = None
    """
    Difference in metric between the current and comparison experiment
    """


@dataclass
class SummarizeExperimentResponse(SerializableDataClass):
    project_name: str
    """
    Name of the project that the experiment belongs to
    """
    experiment_name: str
    """
    Name of the experiment
    """
    project_url: str
    """
    URL to the project's page in the Braintrust app
    """
    experiment_url: str
    """
    URL to the experiment's page in the Braintrust app
    """
    comparison_experiment_name: Optional[str] = None
    """
    The experiment which scores are baselined against
    """
    scores: Optional[Dict[str, ScoreSummary]] = None
    """
    Summary of the experiment's scores
    """
    metrics: Optional[Dict[str, MetricSummary]] = None
    """
    Summary of the experiment's metrics
    """


@dataclass
class Dataset(SerializableDataClass):
    id: str
    """
    Unique identifier for the dataset
    """
    project_id: str
    """
    Unique identifier for the project that the dataset belongs under
    """
    name: str
    """
    Name of the dataset. Within a project, dataset names are unique
    """
    description: Optional[str] = None
    """
    Textual description of the dataset
    """
    created: Optional[str] = None
    """
    Date of dataset creation
    """
    deleted_at: Optional[str] = None
    """
    Date of dataset deletion, or null if the dataset is still active
    """
    user_id: Optional[str] = None
    """
    Identifies the user who created the dataset
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the dataset
    """


@dataclass
class CreateDataset(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project that the dataset belongs under
    """
    name: str
    """
    Name of the dataset. Within a project, dataset names are unique
    """
    description: Optional[str] = None
    """
    Textual description of the dataset
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the dataset
    """


@dataclass
class PatchDataset(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the dataset. Within a project, dataset names are unique
    """
    description: Optional[str] = None
    """
    Textual description of the dataset
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the dataset
    """


class FieldIsMerge4(Enum):
    BOOLEAN_FALSE = False
    BOOLEAN_NONE = None


@dataclass
class InsertDatasetEventReplace(SerializableDataClass):
    input: Optional[Any] = None
    """
    The argument that uniquely define an input case (an arbitrary, JSON serializable object)
    """
    expected: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object)
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    id: Optional[str] = None
    """
    A unique identifier for the dataset event. If you don't provide one, BrainTrust will generate one for you
    """
    created: Optional[str] = None
    """
    The timestamp the dataset event was created
    """
    _object_delete: Optional[bool] = None
    """
    Pass `_object_delete=true` to mark the dataset event deleted. Deleted events will not show up in subsequent fetches for this dataset
    """
    _is_merge: Optional[FieldIsMerge4] = None
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    _parent_id: Optional[str] = None
    """
    Use the `_parent_id` field to create this row as a subspan of an existing row. It cannot be specified alongside `_is_merge=true`. Tracking hierarchical relationships are important for tracing (see the [guide](https://www.braintrust.dev/docs/guides/tracing) for full details).

    For example, say we have logged a row `{"id": "abc", "input": "foo", "output": "bar", "expected": "boo", "scores": {"correctness": 0.33}}`. We can create a sub-span of the parent row by logging `{"_parent_id": "abc", "id": "llm_call", "input": {"prompt": "What comes after foo?"}, "output": "bar", "metrics": {"tokens": 1}}`. In the webapp, only the root span row `"abc"` will show up in the summary view. You can view the full trace hierarchy (in this case, the `"llm_call"` row) by clicking on the "abc" row.
    """


class FieldIsMerge5(Enum):
    BOOLEAN_TRUE = True


@dataclass
class InsertDatasetEventMerge(SerializableDataClass):
    _is_merge: FieldIsMerge5
    """
    The `_is_merge` field controls how the row is merged with any existing row with the same id in the DB. By default (or when set to `false`), the existing row is completely replaced by the new row. When set to `true`, the new row is deep-merged into the existing row

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": 5, "b": 10}}`. If we merge a new row as `{"_is_merge": true, "id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"a": 5, "b": 11, "c": 20}}`. If we replace the new row as `{"id": "foo", "input": {"b": 11, "c": 20}}`, the new row will be `{"id": "foo", "input": {"b": 11, "c": 20}}`
    """
    input: Optional[Any] = None
    """
    The argument that uniquely define an input case (an arbitrary, JSON serializable object)
    """
    expected: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object)
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    id: Optional[str] = None
    """
    A unique identifier for the dataset event. If you don't provide one, BrainTrust will generate one for you
    """
    created: Optional[str] = None
    """
    The timestamp the dataset event was created
    """
    _object_delete: Optional[bool] = None
    """
    Pass `_object_delete=true` to mark the dataset event deleted. Deleted events will not show up in subsequent fetches for this dataset
    """
    _merge_paths: Optional[List[List[str]]] = None
    """
    The `_merge_paths` field allows controlling the depth of the merge. It can only be specified alongside `_is_merge=true`. `_merge_paths` is a list of paths, where each path is a list of field names. The deep merge will not descend below any of the specified merge paths.

    For example, say there is an existing row in the DB `{"id": "foo", "input": {"a": {"b": 10}, "c": {"d": 20}}, "output": {"a": 20}}`. If we merge a new row as `{"_is_merge": true, "_merge_paths": [["input", "a"], ["output"]], "input": {"a": {"q": 30}, "c": {"e": 30}, "bar": "baz"}, "output": {"d": 40}}`, the new row will be `{"id": "foo": "input": {"a": {"q": 30}, "c": {"d": 20, "e": 30}, "bar": "baz"}, "output": {"d": 40}}`. In this case, due to the merge paths, we have replaced `input.a` and `output`, but have still deep-merged `input` and `input.c`.
    """


InsertDatasetEvent = Union[InsertDatasetEventReplace, InsertDatasetEventMerge]


@dataclass
class InsertDatasetEventRequest(SerializableDataClass):
    events: List[InsertDatasetEvent]
    """
    A list of dataset events to insert
    """


class ObjectType4(Enum):
    EXPERIMENT = "experiment"
    DATASET = "dataset"
    PROMPT = "prompt"
    FUNCTION = "function"
    PROMPT_SESSION = "prompt_session"


class ObjectType5(Enum):
    PROJECT_LOGS = "project_logs"


@dataclass
class Origin2(SerializableDataClass):
    object_type: Union[ObjectType4, ObjectType5]
    """
    Type of the object the event is originating from.
    """
    object_id: str
    """
    ID of the object the event is originating from.
    """
    id: str
    """
    ID of the original event.
    """
    _xact_id: str
    """
    Transaction ID of the original event.
    """


@dataclass
class DatasetEvent(SerializableDataClass):
    id: str
    """
    A unique identifier for the dataset event. If you don't provide one, BrainTrust will generate one for you
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the dataset (see the `version` parameter)
    """
    created: str
    """
    The timestamp the dataset event was created
    """
    project_id: str
    """
    Unique identifier for the project that the dataset belongs under
    """
    dataset_id: str
    """
    Unique identifier for the dataset
    """
    span_id: str
    """
    A unique identifier used to link different dataset events together as part of a full trace. See the [tracing guide](https://www.braintrust.dev/docs/guides/tracing) for full details on tracing
    """
    root_span_id: str
    """
    The `span_id` of the root of the trace this dataset event belongs to
    """
    input: Optional[Any] = None
    """
    The argument that uniquely define an input case (an arbitrary, JSON serializable object)
    """
    expected: Optional[Any] = None
    """
    The output of your application, including post-processing (an arbitrary, JSON serializable object)
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings
    """
    tags: Optional[List[str]] = None
    """
    A list of tags to log
    """
    is_root: Optional[bool] = None
    """
    Whether this span is a root span
    """
    origin: Optional[Origin2] = None
    """
    Indicates the event was copied from another object.
    """


@dataclass
class FetchDatasetEventsResponse(SerializableDataClass):
    events: List[DatasetEvent]
    """
    A list of fetched events
    """
    cursor: Optional[str] = None
    """
    Pagination cursor

    Pass this string directly as the `cursor` param to your next fetch request to get the next page of results. Not provided if the returned result set is empty.
    """


class Source2Enum(Enum):
    APP = "app"
    API = "api"
    EXTERNAL = "external"


Source2 = Optional[Source2Enum]


@dataclass
class FeedbackDatasetItem(SerializableDataClass):
    id: str
    """
    The id of the dataset event to log feedback for. This is the row `id` returned by `POST /v1/dataset/{dataset_id}/insert`
    """
    comment: Optional[str] = None
    """
    An optional comment string to log about the dataset event
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    A dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
    """
    source: Optional[Source2] = None
    """
    The source of the feedback. Must be one of "external" (default), "app", or "api"
    """


@dataclass
class FeedbackDatasetEventRequest(SerializableDataClass):
    feedback: List[FeedbackDatasetItem]
    """
    A list of dataset feedback items
    """


@dataclass
class DataSummary(SerializableDataClass):
    total_records: int
    """
    Total number of records in the dataset
    """


@dataclass
class SummarizeDatasetResponse(SerializableDataClass):
    project_name: str
    """
    Name of the project that the dataset belongs to
    """
    dataset_name: str
    """
    Name of the dataset
    """
    project_url: str
    """
    URL to the project's page in the Braintrust app
    """
    dataset_url: str
    """
    URL to the dataset's page in the Braintrust app
    """
    data_summary: Optional[DataSummary] = None


class Type7(Enum):
    TEXT = "text"


@dataclass
class ChatCompletionContentPartText(SerializableDataClass):
    type: Type7
    text: Optional[str] = ""


class Detail(Enum):
    AUTO = "auto"


class Detail1(Enum):
    LOW = "low"


class Detail2(Enum):
    HIGH = "high"


@dataclass
class ImageUrl(SerializableDataClass):
    url: str
    detail: Optional[Union[Detail, Detail1, Detail2]] = None


class Type8(Enum):
    IMAGE_URL = "image_url"


@dataclass
class ChatCompletionContentPartImage(SerializableDataClass):
    image_url: ImageUrl
    type: Type8


ChatCompletionContentPart = Union[ChatCompletionContentPartText, ChatCompletionContentPartImage]


ChatCompletionContent = Union[str, List[ChatCompletionContentPart]]


@dataclass
class Function(SerializableDataClass):
    arguments: str
    name: str


class Type9(Enum):
    FUNCTION = "function"


@dataclass
class ChatCompletionMessageToolCall(SerializableDataClass):
    id: str
    function: Function
    type: Type9


class Role(Enum):
    SYSTEM = "system"


@dataclass
class ChatCompletionMessageParam1(SerializableDataClass):
    role: Role
    content: Optional[str] = ""
    name: Optional[str] = None


class Role1(Enum):
    USER = "user"


@dataclass
class ChatCompletionMessageParam2(SerializableDataClass):
    role: Role1
    content: Optional[ChatCompletionContent] = None
    name: Optional[str] = None


class Role2(Enum):
    ASSISTANT = "assistant"


@dataclass
class FunctionCall(SerializableDataClass):
    arguments: str
    name: str


@dataclass
class ChatCompletionMessageParam3(SerializableDataClass):
    role: Role2
    content: Optional[str] = None
    function_call: Optional[FunctionCall] = None
    name: Optional[str] = None
    tool_calls: Optional[List[ChatCompletionMessageToolCall]] = None


class Role3(Enum):
    TOOL = "tool"


@dataclass
class ChatCompletionMessageParam4(SerializableDataClass):
    role: Role3
    content: Optional[str] = ""
    tool_call_id: Optional[str] = ""


class Role4(Enum):
    FUNCTION = "function"


@dataclass
class ChatCompletionMessageParam5(SerializableDataClass):
    name: str
    role: Role4
    content: Optional[str] = ""


class Role5(Enum):
    MODEL = "model"


@dataclass
class ChatCompletionMessageParam6(SerializableDataClass):
    role: Role5
    content: Optional[str] = None


ChatCompletionMessageParam = Union[
    ChatCompletionMessageParam1,
    ChatCompletionMessageParam2,
    ChatCompletionMessageParam3,
    ChatCompletionMessageParam4,
    ChatCompletionMessageParam5,
    ChatCompletionMessageParam6,
]


class Type10(Enum):
    JSON_OBJECT = "json_object"


@dataclass
class ResponseFormat(SerializableDataClass):
    type: Type10


class Type11(Enum):
    JSON_SCHEMA = "json_schema"


@dataclass
class JsonSchema(SerializableDataClass):
    name: str
    description: Optional[str] = None
    schema_: Optional[Dict[str, Any]] = None
    strict: Optional[bool] = None


@dataclass
class ResponseFormat1(SerializableDataClass):
    type: Type11
    json_schema: JsonSchema


class Type12(Enum):
    TEXT = "text"


@dataclass
class ResponseFormat2(SerializableDataClass):
    type: Type12


class ToolChoice(Enum):
    AUTO = "auto"


class ToolChoice1(Enum):
    NONE = "none"


class ToolChoice2(Enum):
    REQUIRED = "required"


class Type13(Enum):
    FUNCTION = "function"


@dataclass
class Function1(SerializableDataClass):
    name: str


@dataclass
class ToolChoice3(SerializableDataClass):
    type: Type13
    function: Function1


class FunctionCall1(Enum):
    AUTO = "auto"


class FunctionCall2(Enum):
    NONE = "none"


@dataclass
class FunctionCall3(SerializableDataClass):
    name: str


@dataclass
class ModelParams1(SerializableDataClass):
    use_cache: Optional[bool] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    max_tokens: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    response_format: Optional[Union[ResponseFormat, ResponseFormat1, ResponseFormat2, Dict[str, Any]]] = None
    tool_choice: Optional[Union[ToolChoice, ToolChoice1, ToolChoice2, ToolChoice3]] = None
    function_call: Optional[Union[FunctionCall1, FunctionCall2, FunctionCall3]] = None
    n: Optional[float] = None
    stop: Optional[List[str]] = None


@dataclass
class ModelParams2(SerializableDataClass):
    max_tokens: float
    temperature: float
    use_cache: Optional[bool] = None
    top_p: Optional[float] = None
    top_k: Optional[float] = None
    stop_sequences: Optional[List[str]] = None
    max_tokens_to_sample: Optional[float] = None
    """
    This is a legacy parameter that should not be used.
    """


@dataclass
class ModelParams3(SerializableDataClass):
    use_cache: Optional[bool] = None
    temperature: Optional[float] = None
    maxOutputTokens: Optional[float] = None
    topP: Optional[float] = None
    topK: Optional[float] = None


@dataclass
class ModelParams4(SerializableDataClass):
    use_cache: Optional[bool] = None
    temperature: Optional[float] = None
    topK: Optional[float] = None


@dataclass
class ModelParams5(SerializableDataClass):
    use_cache: Optional[bool] = None


ModelParams = Union[ModelParams1, ModelParams2, ModelParams3, ModelParams4, ModelParams5]


@dataclass
class SavedFunctionId1(SerializableDataClass):
    type: Type13
    id: str


class Type15(Enum):
    GLOBAL_ = "global"


@dataclass
class SavedFunctionId2(SerializableDataClass):
    type: Type15
    name: str


SavedFunctionId = Union[SavedFunctionId1, SavedFunctionId2]


class Type16(Enum):
    COMPLETION = "completion"


@dataclass
class Prompt(SerializableDataClass):
    type: Type16
    content: str


class Type17(Enum):
    CHAT = "chat"


@dataclass
class Prompt1(SerializableDataClass):
    type: Type17
    messages: List[ChatCompletionMessageParam]
    tools: Optional[str] = None


@dataclass
class Options(SerializableDataClass):
    model: Optional[str] = None
    params: Optional[ModelParams] = None
    position: Optional[str] = None


class Type18(Enum):
    LLM_CLASSIFIER = "llm_classifier"


@dataclass
class Parser(SerializableDataClass):
    type: Type18
    use_cot: bool
    choice_scores: Dict[str, float]


@dataclass
class Origin3(SerializableDataClass):
    prompt_id: Optional[str] = None
    project_id: Optional[str] = None
    prompt_version: Optional[str] = None


@dataclass
class PromptData(SerializableDataClass):
    prompt: Optional[Union[Prompt, Prompt1, Dict[str, Any]]] = None
    options: Optional[Options] = None
    parser: Optional[Parser] = None
    tool_functions: Optional[List[SavedFunctionId]] = None
    origin: Optional[Origin3] = None


class LogId1(Enum):
    P = "p"


class FunctionTypeEnum(Enum):
    LLM = "llm"
    SCORER = "scorer"
    TASK = "task"
    TOOL = "tool"


FunctionType = Optional[FunctionTypeEnum]


@dataclass
class Prompt2(SerializableDataClass):
    id: str
    """
    Unique identifier for the prompt
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the prompt (see the `version` parameter)
    """
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    log_id: LogId1
    """
    A literal 'p' which identifies the object as a project prompt
    """
    org_id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    description: Optional[str] = None
    """
    Textual description of the prompt
    """
    created: Optional[str] = None
    """
    Date of prompt creation
    """
    prompt_data: Optional[PromptData] = None
    tags: Optional[List[str]] = None
    """
    A list of tags for the prompt
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the prompt
    """
    function_type: Optional[FunctionType] = None


class FunctionType1Enum(Enum):
    LLM = "llm"
    SCORER = "scorer"
    TASK = "task"
    TOOL = "tool"


FunctionType1 = Optional[FunctionType1Enum]


@dataclass
class CreatePrompt(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    description: Optional[str] = None
    """
    Textual description of the prompt
    """
    prompt_data: Optional[PromptData] = None
    tags: Optional[List[str]] = None
    """
    A list of tags for the prompt
    """
    function_type: Optional[FunctionType1] = None


@dataclass
class PatchPrompt(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the prompt
    """
    slug: Optional[str] = None
    """
    Unique identifier for the prompt
    """
    description: Optional[str] = None
    """
    Textual description of the prompt
    """
    prompt_data: Optional[PromptData] = None
    tags: Optional[List[str]] = None
    """
    A list of tags for the prompt
    """


class Permission(Enum):
    CREATE = "create"
    READ = "read"
    UPDATE = "update"
    DELETE = "delete"
    CREATE_ACLS = "create_acls"
    READ_ACLS = "read_acls"
    UPDATE_ACLS = "update_acls"
    DELETE_ACLS = "delete_acls"


@dataclass
class MemberPermission(SerializableDataClass):
    permission: Permission
    restrict_object_type: Optional[AclObjectType] = None


@dataclass
class Role6(SerializableDataClass):
    id: str
    """
    Unique identifier for the role
    """
    name: str
    """
    Name of the role
    """
    org_id: Optional[str] = None
    """
    Unique id for the organization that the role belongs under

    A null org_id indicates a system role, which may be assigned to anybody and inherited by any other role, but cannot be edited.

    It is forbidden to change the org after creating a role
    """
    user_id: Optional[str] = None
    """
    Identifies the user who created the role
    """
    created: Optional[str] = None
    """
    Date of role creation
    """
    description: Optional[str] = None
    """
    Textual description of the role
    """
    deleted_at: Optional[str] = None
    """
    Date of role deletion, or null if the role is still active
    """
    member_permissions: Optional[List[MemberPermission]] = None
    """
    (permission, restrict_object_type) tuples which belong to this role
    """
    member_roles: Optional[List[str]] = None
    """
    Ids of the roles this role inherits from

    An inheriting role has all the permissions contained in its member roles, as well as all of their inherited permissions
    """


@dataclass
class CreateRole(SerializableDataClass):
    name: str
    """
    Name of the role
    """
    description: Optional[str] = None
    """
    Textual description of the role
    """
    member_permissions: Optional[List[MemberPermission]] = None
    """
    (permission, restrict_object_type) tuples which belong to this role
    """
    member_roles: Optional[List[str]] = None
    """
    Ids of the roles this role inherits from

    An inheriting role has all the permissions contained in its member roles, as well as all of their inherited permissions
    """
    org_name: Optional[str] = None
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the role belongs in.
    """


@dataclass
class AddMemberPermission(SerializableDataClass):
    permission: Permission
    restrict_object_type: Optional[AclObjectType] = None


@dataclass
class RemoveMemberPermission(SerializableDataClass):
    permission: Permission
    restrict_object_type: Optional[AclObjectType] = None


@dataclass
class PatchRole(SerializableDataClass):
    description: Optional[str] = None
    """
    Textual description of the role
    """
    name: Optional[str] = None
    """
    Name of the role
    """
    add_member_permissions: Optional[List[AddMemberPermission]] = None
    """
    A list of permissions to add to the role
    """
    remove_member_permissions: Optional[List[RemoveMemberPermission]] = None
    """
    A list of permissions to remove from the role
    """
    add_member_roles: Optional[List[str]] = None
    """
    A list of role IDs to add to the role's inheriting-from set
    """
    remove_member_roles: Optional[List[str]] = None
    """
    A list of role IDs to remove from the role's inheriting-from set
    """


@dataclass
class Group(SerializableDataClass):
    id: str
    """
    Unique identifier for the group
    """
    org_id: str
    """
    Unique id for the organization that the group belongs under

    It is forbidden to change the org after creating a group
    """
    name: str
    """
    Name of the group
    """
    user_id: Optional[str] = None
    """
    Identifies the user who created the group
    """
    created: Optional[str] = None
    """
    Date of group creation
    """
    description: Optional[str] = None
    """
    Textual description of the group
    """
    deleted_at: Optional[str] = None
    """
    Date of group deletion, or null if the group is still active
    """
    member_users: Optional[List[str]] = None
    """
    Ids of users which belong to this group
    """
    member_groups: Optional[List[str]] = None
    """
    Ids of the groups this group inherits from

    An inheriting group has all the users contained in its member groups, as well as all of their inherited users
    """


@dataclass
class CreateGroup(SerializableDataClass):
    name: str
    """
    Name of the group
    """
    description: Optional[str] = None
    """
    Textual description of the group
    """
    member_users: Optional[List[str]] = None
    """
    Ids of users which belong to this group
    """
    member_groups: Optional[List[str]] = None
    """
    Ids of the groups this group inherits from

    An inheriting group has all the users contained in its member groups, as well as all of their inherited users
    """
    org_name: Optional[str] = None
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the group belongs in.
    """


@dataclass
class PatchGroup(SerializableDataClass):
    description: Optional[str] = None
    """
    Textual description of the group
    """
    name: Optional[str] = None
    """
    Name of the group
    """
    add_member_users: Optional[List[str]] = None
    """
    A list of user IDs to add to the group
    """
    remove_member_users: Optional[List[str]] = None
    """
    A list of user IDs to remove from the group
    """
    add_member_groups: Optional[List[str]] = None
    """
    A list of group IDs to add to the group's inheriting-from set
    """
    remove_member_groups: Optional[List[str]] = None
    """
    A list of group IDs to remove from the group's inheriting-from set
    """


@dataclass
class Acl(SerializableDataClass):
    id: str
    """
    Unique identifier for the acl
    """
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the ACL applies to
    """
    _object_org_id: str
    """
    The organization the ACL's referred object belongs to
    """
    user_id: Optional[str] = None
    """
    Id of the user the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    group_id: Optional[str] = None
    """
    Id of the group the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    permission: Optional[Permission] = None
    restrict_object_type: Optional[AclObjectType] = None
    role_id: Optional[str] = None
    """
    Id of the role the ACL grants. Exactly one of `permission` and `role_id` will be provided
    """
    created: Optional[str] = None
    """
    Date of acl creation
    """


@dataclass
class AclItem(SerializableDataClass):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the ACL applies to
    """
    user_id: Optional[str] = None
    """
    Id of the user the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    group_id: Optional[str] = None
    """
    Id of the group the ACL applies to. Exactly one of `user_id` and `group_id` will be provided
    """
    permission: Optional[Permission] = None
    restrict_object_type: Optional[AclObjectType] = None
    role_id: Optional[str] = None
    """
    Id of the role the ACL grants. Exactly one of `permission` and `role_id` will be provided
    """


@dataclass
class AclBatchUpdateResponse(SerializableDataClass):
    added_acls: List[Acl]
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """
    removed_acls: List[Acl]
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """


@dataclass
class AclBatchUpdateRequest(SerializableDataClass):
    add_acls: Optional[List[AclItem]] = None
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """
    remove_acls: Optional[List[AclItem]] = None
    """
    An ACL grants a certain permission or role to a certain user or group on an object.

    ACLs are inherited across the object hierarchy. So for example, if a user has read permissions on a project, they will also have read permissions on any experiment, dataset, etc. created within that project.

    To restrict a grant to a particular sub-object, you may specify `restrict_object_type` in the ACL, as part of a direct permission grant or as part of a role.
    """


@dataclass
class User(SerializableDataClass):
    id: str
    """
    Unique identifier for the user
    """
    given_name: Optional[str] = None
    """
    Given name of the user
    """
    family_name: Optional[str] = None
    """
    Family name of the user
    """
    email: Optional[str] = None
    """
    The user's email
    """
    avatar_url: Optional[str] = None
    """
    URL of the user's Avatar image
    """
    created: Optional[str] = None
    """
    Date of user creation
    """


@dataclass
class ProjectScoreCategory(SerializableDataClass):
    name: str
    """
    Name of the category
    """
    value: float
    """
    Numerical value of the category. Must be between 0 and 1, inclusive
    """


ProjectScoreCategories = Union[List[ProjectScoreCategory], Dict[str, float], List[str], Dict[str, Any]]


@dataclass
class OnlineScoreConfig(SerializableDataClass):
    sampling_rate: float
    """
    The sampling rate for online scoring
    """
    scorers: List[SavedFunctionId]
    """
    The list of scorers to use for online scoring
    """
    apply_to_root_span: Optional[bool] = None
    """
    Whether to trigger online scoring on the root span of each trace
    """
    apply_to_span_names: Optional[List[str]] = None
    """
    Trigger online scoring on any spans with a name in this list
    """


class DestinationEnum(Enum):
    EXPECTED = "expected"


Destination = Optional[DestinationEnum]


@dataclass
class ProjectScoreConfig(SerializableDataClass):
    multi_select: Optional[bool] = None
    destination: Optional[Destination] = None
    online: Optional[OnlineScoreConfig] = None


@dataclass
class ProjectScore(SerializableDataClass):
    id: str
    """
    Unique identifier for the project score
    """
    project_id: str
    """
    Unique identifier for the project that the project score belongs under
    """
    user_id: str
    name: str
    """
    Name of the project score
    """
    score_type: ProjectScoreType
    created: Optional[str] = None
    """
    Date of project score creation
    """
    description: Optional[str] = None
    """
    Textual description of the project score
    """
    categories: Optional[ProjectScoreCategories] = None
    config: Optional[ProjectScoreConfig] = None
    position: Optional[str] = None
    """
    An optional LexoRank-based string that sets the sort position for the score in the UI
    """


@dataclass
class CreateProjectScore(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project that the project score belongs under
    """
    name: str
    """
    Name of the project score
    """
    score_type: ProjectScoreType
    description: Optional[str] = None
    """
    Textual description of the project score
    """
    categories: Optional[ProjectScoreCategories] = None
    config: Optional[ProjectScoreConfig] = None


@dataclass
class PatchProjectScore(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the project score
    """
    description: Optional[str] = None
    """
    Textual description of the project score
    """
    score_type: Optional[ProjectScoreType] = None
    categories: Optional[ProjectScoreCategories] = None
    config: Optional[ProjectScoreConfig] = None


@dataclass
class ProjectTag(SerializableDataClass):
    id: str
    """
    Unique identifier for the project tag
    """
    project_id: str
    """
    Unique identifier for the project that the project tag belongs under
    """
    user_id: str
    name: str
    """
    Name of the project tag
    """
    created: Optional[str] = None
    """
    Date of project tag creation
    """
    description: Optional[str] = None
    """
    Textual description of the project tag
    """
    color: Optional[str] = None
    """
    Color of the tag for the UI
    """


@dataclass
class CreateProjectTag(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project that the project tag belongs under
    """
    name: str
    """
    Name of the project tag
    """
    description: Optional[str] = None
    """
    Textual description of the project tag
    """
    color: Optional[str] = None
    """
    Color of the tag for the UI
    """


@dataclass
class PatchProjectTag(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the project tag
    """
    description: Optional[str] = None
    """
    Textual description of the project tag
    """
    color: Optional[str] = None
    """
    Color of the tag for the UI
    """


@dataclass
class SpanIFrame(SerializableDataClass):
    id: str
    """
    Unique identifier for the span iframe
    """
    project_id: str
    """
    Unique identifier for the project that the span iframe belongs under
    """
    name: str
    """
    Name of the span iframe
    """
    url: str
    """
    URL to embed the project viewer in an iframe
    """
    user_id: Optional[str] = None
    """
    Identifies the user who created the span iframe
    """
    created: Optional[str] = None
    """
    Date of span iframe creation
    """
    deleted_at: Optional[str] = None
    """
    Date of span iframe deletion, or null if the span iframe is still active
    """
    description: Optional[str] = None
    """
    Textual description of the span iframe
    """
    post_message: Optional[bool] = None
    """
    Whether to post messages to the iframe containing the span's data. This is useful when you want to render more data than fits in the URL.
    """


@dataclass
class CreateSpanIFrame(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project that the span iframe belongs under
    """
    name: str
    """
    Name of the span iframe
    """
    url: str
    """
    URL to embed the project viewer in an iframe
    """
    description: Optional[str] = None
    """
    Textual description of the span iframe
    """
    post_message: Optional[bool] = None
    """
    Whether to post messages to the iframe containing the span's data. This is useful when you want to render more data than fits in the URL.
    """


@dataclass
class PatchSpanIFrame(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the span iframe
    """
    url: Optional[str] = None
    """
    URL to embed the project viewer in an iframe
    """
    post_message: Optional[bool] = None
    """
    Whether to post messages to the iframe containing the span's data. This is useful when you want to render more data than fits in the URL.
    """


class Runtime(Enum):
    NODE = "node"
    PYTHON = "python"


@dataclass
class RuntimeContext(SerializableDataClass):
    runtime: Runtime
    version: str


class Type19(Enum):
    EXPERIMENT = "experiment"


class Type20(Enum):
    TASK = "task"


@dataclass
class Position(SerializableDataClass):
    type: Type20


class Type21(Enum):
    SCORER = "scorer"


@dataclass
class Position1(SerializableDataClass):
    type: Type21
    index: int


@dataclass
class Location(SerializableDataClass):
    type: Type19
    eval_name: str
    position: Union[Position, Position1]


class Type22(Enum):
    FUNCTION = "function"


@dataclass
class Location1(SerializableDataClass):
    type: Type22
    index: int


@dataclass
class CodeBundle(SerializableDataClass):
    runtime_context: RuntimeContext
    location: Union[Location, Location1]
    bundle_id: str
    preview: Optional[str]
    """
    A preview of the code
    """


class Type23(Enum):
    PROMPT = "prompt"


@dataclass
class FunctionData1(SerializableDataClass):
    type: Type23


class Type24(Enum):
    CODE = "code"


class Type25(Enum):
    BUNDLE = "bundle"


@dataclass
class Data(CodeBundle):
    type: Type25


class Type26(Enum):
    INLINE = "inline"


@dataclass
class RuntimeContext1(SerializableDataClass):
    runtime: Runtime
    version: str


@dataclass
class Data1(SerializableDataClass):
    type: Type26
    runtime_context: RuntimeContext1
    code: str


@dataclass
class FunctionData2(SerializableDataClass):
    type: Type24
    data: Union[Data, Data1]


class Type27(Enum):
    GLOBAL_ = "global"


@dataclass
class FunctionData3(SerializableDataClass):
    type: Type27
    name: str


FunctionData = Union[FunctionData1, FunctionData2, FunctionData3]


class FunctionType2Enum(Enum):
    LLM = "llm"
    SCORER = "scorer"
    TASK = "task"
    TOOL = "tool"


FunctionType2 = Optional[FunctionType2Enum]


@dataclass
class Origin4(SerializableDataClass):
    object_type: AclObjectType
    object_id: str
    """
    Id of the object the function is originating from
    """
    internal: Optional[bool] = None
    """
    The function exists for internal purposes and should not be displayed in the list of functions.
    """


@dataclass
class FunctionSchema(SerializableDataClass):
    parameters: Optional[Any] = None
    returns: Optional[Any] = None


@dataclass
class Function2(SerializableDataClass):
    id: str
    """
    Unique identifier for the prompt
    """
    _xact_id: str
    """
    The transaction id of an event is unique to the network operation that processed the event insertion. Transaction ids are monotonically increasing over time and can be used to retrieve a versioned snapshot of the prompt (see the `version` parameter)
    """
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    log_id: LogId1
    """
    A literal 'p' which identifies the object as a project prompt
    """
    org_id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    function_data: FunctionData
    description: Optional[str] = None
    """
    Textual description of the prompt
    """
    created: Optional[str] = None
    """
    Date of prompt creation
    """
    prompt_data: Optional[PromptData] = None
    tags: Optional[List[str]] = None
    """
    A list of tags for the prompt
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    User-controlled metadata about the prompt
    """
    function_type: Optional[FunctionType2] = None
    origin: Optional[Origin4] = None
    function_schema: Optional[FunctionSchema] = None
    """
    JSON schema for the function's parameters and return type
    """


class FunctionType3Enum(Enum):
    LLM = "llm"
    SCORER = "scorer"
    TASK = "task"
    TOOL = "tool"


FunctionType3 = Optional[FunctionType3Enum]


@dataclass
class CreateFunction(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project that the prompt belongs under
    """
    name: str
    """
    Name of the prompt
    """
    slug: str
    """
    Unique identifier for the prompt
    """
    function_data: FunctionData
    description: Optional[str] = None
    """
    Textual description of the prompt
    """
    prompt_data: Optional[PromptData] = None
    tags: Optional[List[str]] = None
    """
    A list of tags for the prompt
    """
    function_type: Optional[FunctionType3] = None
    origin: Optional[Origin4] = None
    function_schema: Optional[FunctionSchema] = None
    """
    JSON schema for the function's parameters and return type
    """


class Type28(Enum):
    PROMPT = "prompt"


@dataclass
class FunctionDataNullish1(SerializableDataClass):
    type: Type28


class Type29(Enum):
    CODE = "code"


class Type30(Enum):
    BUNDLE = "bundle"


@dataclass
class Data2(CodeBundle):
    type: Type30


class Type31(Enum):
    INLINE = "inline"


@dataclass
class RuntimeContext2(SerializableDataClass):
    runtime: Runtime
    version: str


@dataclass
class Data3(SerializableDataClass):
    type: Type31
    runtime_context: RuntimeContext2
    code: str


@dataclass
class FunctionDataNullish2(SerializableDataClass):
    type: Type29
    data: Union[Data2, Data3]


class Type32(Enum):
    GLOBAL_ = "global"


@dataclass
class FunctionDataNullish3(SerializableDataClass):
    type: Type32
    name: str


FunctionDataNullish = Union[FunctionDataNullish1, FunctionDataNullish2, FunctionDataNullish3, Dict[str, Any]]


@dataclass
class PatchFunction(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the prompt
    """
    description: Optional[str] = None
    """
    Textual description of the prompt
    """
    prompt_data: Optional[PromptData] = None
    function_data: Optional[FunctionDataNullish] = None
    tags: Optional[List[str]] = None
    """
    A list of tags for the prompt
    """


class ObjectType6(Enum):
    PROJECT_LOGS = "project_logs"
    EXPERIMENT = "experiment"


@dataclass
class RowIds(SerializableDataClass):
    id: str
    """
    The id of the row
    """
    span_id: str
    """
    The span_id of the row
    """
    root_span_id: str
    """
    The root_span_id of the row
    """


@dataclass
class Parent(SerializableDataClass):
    object_type: ObjectType6
    object_id: str
    """
    The id of the container object you are logging to
    """
    row_ids: Optional[RowIds] = None
    """
    Identifiers for the row to to log a subspan under
    """
    propagated_event: Optional[Dict[str, Any]] = None
    """
    Include these properties in every span created under this parent
    """


class ModeEnum(Enum):
    AUTO = "auto"
    PARALLEL = "parallel"


Mode = Optional[ModeEnum]


@dataclass
class InvokeApi(SerializableDataClass):
    input: Optional[Any] = None
    """
    Argument to the function, which can be any JSON serializable value
    """
    messages: Optional[List[ChatCompletionMessageParam]] = None
    """
    If the function is an LLM, additional messages to pass along to it
    """
    parent: Optional[Union[Parent, str]] = None
    """
    Options for tracing the function call
    """
    stream: Optional[bool] = None
    """
    Whether to stream the response. If true, results will be returned in the Braintrust SSE format.
    """
    mode: Optional[Mode] = None
    """
    The mode format of the returned value (defaults to 'auto')
    """
    version: Optional[str] = None
    """
    The version of the function
    """


@dataclass
class ViewDataSearch(SerializableDataClass):
    filter: Optional[List[Any]] = None
    tag: Optional[List[Any]] = None
    match: Optional[List[Any]] = None
    sort: Optional[List[Any]] = None


@dataclass
class ViewData(SerializableDataClass):
    search: Optional[ViewDataSearch] = None


@dataclass
class ViewOptions(SerializableDataClass):
    columnVisibility: Optional[Dict[str, bool]] = None
    columnOrder: Optional[List[str]] = None
    columnSizing: Optional[Dict[str, float]] = None


class ViewType1Enum(Enum):
    PROJECTS = "projects"
    LOGS = "logs"
    EXPERIMENTS = "experiments"
    DATASETS = "datasets"
    PROMPTS = "prompts"
    PLAYGROUNDS = "playgrounds"
    EXPERIMENT = "experiment"
    DATASET = "dataset"


ViewType1 = Optional[ViewType1Enum]


@dataclass
class View(SerializableDataClass):
    id: str
    """
    Unique identifier for the view
    """
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """
    view_type: ViewType1
    """
    Type of table that the view corresponds to.
    """
    name: str
    """
    Name of the view
    """
    created: Optional[str] = None
    """
    Date of view creation
    """
    view_data: Optional[ViewData] = None
    options: Optional[ViewOptions] = None
    user_id: Optional[str] = None
    """
    Identifies the user who created the view
    """
    deleted_at: Optional[str] = None
    """
    Date of role deletion, or null if the role is still active
    """


class ViewType2Enum(Enum):
    PROJECTS = "projects"
    LOGS = "logs"
    EXPERIMENTS = "experiments"
    DATASETS = "datasets"
    PROMPTS = "prompts"
    PLAYGROUNDS = "playgrounds"
    EXPERIMENT = "experiment"
    DATASET = "dataset"


ViewType2 = Optional[ViewType2Enum]


@dataclass
class CreateView(SerializableDataClass):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """
    view_type: ViewType2
    """
    Type of table that the view corresponds to.
    """
    name: str
    """
    Name of the view
    """
    view_data: Optional[ViewData] = None
    options: Optional[ViewOptions] = None
    user_id: Optional[str] = None
    """
    Identifies the user who created the view
    """
    deleted_at: Optional[str] = None
    """
    Date of role deletion, or null if the role is still active
    """


class ViewType3Enum(Enum):
    PROJECTS = "projects"
    LOGS = "logs"
    EXPERIMENTS = "experiments"
    DATASETS = "datasets"
    PROMPTS = "prompts"
    PLAYGROUNDS = "playgrounds"
    EXPERIMENT = "experiment"
    DATASET = "dataset"


ViewType3 = Optional[ViewType3Enum]


@dataclass
class PatchView(SerializableDataClass):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """
    view_type: Optional[ViewType3] = None
    """
    Type of table that the view corresponds to.
    """
    name: Optional[str] = None
    """
    Name of the view
    """
    view_data: Optional[ViewData] = None
    options: Optional[ViewOptions] = None
    user_id: Optional[str] = None
    """
    Identifies the user who created the view
    """


@dataclass
class DeleteView(SerializableDataClass):
    object_type: AclObjectType
    object_id: str
    """
    The id of the object the view applies to
    """


@dataclass
class Organization(SerializableDataClass):
    id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the organization
    """
    api_url: Optional[str] = None
    is_universal_api: Optional[bool] = None
    proxy_url: Optional[str] = None
    realtime_url: Optional[str] = None
    created: Optional[str] = None
    """
    Date of organization creation
    """


@dataclass
class PatchOrganization(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the organization
    """
    api_url: Optional[str] = None
    is_universal_api: Optional[bool] = None
    proxy_url: Optional[str] = None
    realtime_url: Optional[str] = None


@dataclass
class PatchOrganizationMembersOutput(SerializableDataClass):
    status: Status
    send_email_error: Optional[str] = None
    """
    If invite emails failed to send for some reason, the patch operation will still complete, but we will return an error message here
    """


@dataclass
class InviteUsers(SerializableDataClass):
    ids: Optional[List[str]] = None
    """
    Ids of existing users to invite
    """
    emails: Optional[List[str]] = None
    """
    Emails of users to invite
    """
    send_invite_emails: Optional[bool] = None
    """
    If true, send invite emails to the users who wore actually added
    """
    group_ids: Optional[List[str]] = None
    """
    Optional list of group ids to add newly-invited users to.
    """
    group_names: Optional[List[str]] = None
    """
    Optional list of group names to add newly-invited users to.
    """
    group_id: Optional[str] = None
    """
    Singular form of group_ids
    """
    group_name: Optional[str] = None
    """
    Singular form of group_names
    """


@dataclass
class RemoveUsers(SerializableDataClass):
    ids: Optional[List[str]] = None
    """
    Ids of users to remove
    """
    emails: Optional[List[str]] = None
    """
    Emails of users to remove
    """


@dataclass
class PatchOrganizationMembers(SerializableDataClass):
    invite_users: Optional[InviteUsers] = None
    """
    Users to invite to the organization
    """
    remove_users: Optional[RemoveUsers] = None
    """
    Users to remove from the organization
    """
    org_name: Optional[str] = None
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, or in case you want to explicitly assert the organization you are modifying, you may specify the name of the organization.
    """
    org_id: Optional[str] = None
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, or in case you want to explicitly assert the organization you are modifying, you may specify the id of the organization.
    """


@dataclass
class CreateApiKeyOutput(SerializableDataClass):
    id: str
    """
    Unique identifier for the api key
    """
    name: str
    """
    Name of the api key
    """
    preview_name: str
    key: str
    """
    The raw API key. It will only be exposed this one time
    """
    created: Optional[str] = None
    """
    Date of api key creation
    """
    user_id: Optional[str] = None
    """
    Unique identifier for the user
    """
    org_id: Optional[str] = None
    """
    Unique identifier for the organization
    """


@dataclass
class ApiKey(SerializableDataClass):
    id: str
    """
    Unique identifier for the api key
    """
    name: str
    """
    Name of the api key
    """
    preview_name: str
    created: Optional[str] = None
    """
    Date of api key creation
    """
    user_id: Optional[str] = None
    """
    Unique identifier for the user
    """
    org_id: Optional[str] = None
    """
    Unique identifier for the organization
    """


@dataclass
class AISecret(SerializableDataClass):
    id: str
    """
    Unique identifier for the AI secret
    """
    org_id: str
    """
    Unique identifier for the organization
    """
    name: str
    """
    Name of the AI secret
    """
    created: Optional[str] = None
    """
    Date of AI secret creation
    """
    type: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    preview_secret: Optional[str] = None


@dataclass
class CreateAISecret(SerializableDataClass):
    name: str
    """
    Name of the AI secret
    """
    type: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    secret: Optional[str] = None
    """
    Secret value. If omitted in a PUT request, the existing secret value will be left intact, not replaced with null.
    """
    org_name: Optional[str] = None
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the AI Secret belongs in.
    """


@dataclass
class DeleteAISecret(SerializableDataClass):
    name: str
    """
    Name of the AI secret
    """
    org_name: Optional[str] = None
    """
    For nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the AI Secret belongs in.
    """


@dataclass
class PatchAISecret(SerializableDataClass):
    name: Optional[str] = None
    """
    Name of the AI secret
    """
    type: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    secret: Optional[str] = None


class ObjectType7(Enum):
    ORGANIZATION = "organization"
    PROJECT = "project"
    FUNCTION = "function"


@dataclass
class EnvVar(SerializableDataClass):
    id: str
    """
    Unique identifier for the environment variable
    """
    object_type: ObjectType7
    """
    The type of the object the environment variable is scoped for
    """
    object_id: str
    """
    The id of the object the environment variable is scoped for
    """
    name: str
    """
    The name of the environment variable
    """
    created: Optional[str] = None
    """
    Date of environment variable creation
    """
    used: Optional[str] = None
    """
    Date the environment variable was last used
    """


@dataclass
class CrossObjectInsertResponse(SerializableDataClass):
    experiment: Optional[Dict[str, InsertEventsResponse]] = None
    """
    A mapping from experiment id to row ids for inserted `events`
    """
    dataset: Optional[Dict[str, InsertEventsResponse]] = None
    """
    A mapping from dataset id to row ids for inserted `events`
    """
    project_logs: Optional[Dict[str, InsertEventsResponse]] = None
    """
    A mapping from project id to row ids for inserted `events`
    """


@dataclass
class Experiment1(SerializableDataClass):
    events: Optional[List[InsertExperimentEvent]] = None
    """
    A list of experiment events to insert
    """
    feedback: Optional[List[FeedbackExperimentItem]] = None
    """
    A list of experiment feedback items
    """


@dataclass
class Dataset1(SerializableDataClass):
    events: Optional[List[InsertDatasetEvent]] = None
    """
    A list of dataset events to insert
    """
    feedback: Optional[List[FeedbackDatasetItem]] = None
    """
    A list of dataset feedback items
    """


@dataclass
class ProjectLogs(SerializableDataClass):
    events: Optional[List[InsertProjectLogsEvent]] = None
    """
    A list of project logs events to insert
    """
    feedback: Optional[List[FeedbackProjectLogsItem]] = None
    """
    A list of project logs feedback items
    """


@dataclass
class CrossObjectInsertRequest(SerializableDataClass):
    experiment: Optional[Dict[str, Experiment1]] = None
    """
    A mapping from experiment id to a set of log events and feedback items to insert
    """
    dataset: Optional[Dict[str, Dataset1]] = None
    """
    A mapping from dataset id to a set of log events and feedback items to insert
    """
    project_logs: Optional[Dict[str, ProjectLogs]] = None
    """
    A mapping from project id to a set of log events and feedback items to insert
    """


@dataclass
class FunctionId1(SerializableDataClass):
    function_id: str
    """
    The ID of the function
    """
    version: Optional[str] = None
    """
    The version of the function
    """


@dataclass
class FunctionId2(SerializableDataClass):
    project_name: str
    """
    The name of the project containing the function
    """
    slug: str
    """
    The slug of the function
    """
    version: Optional[str] = None
    """
    The version of the function
    """


@dataclass
class FunctionId3(SerializableDataClass):
    global_function: str
    """
    The name of the global function. Currently, the global namespace includes the functions in autoevals
    """


@dataclass
class FunctionId4(SerializableDataClass):
    prompt_session_id: str
    """
    The ID of the prompt session
    """
    prompt_session_function_id: str
    """
    The ID of the function in the prompt session
    """
    version: Optional[str] = None
    """
    The version of the function
    """


@dataclass
class InlineContext(SerializableDataClass):
    runtime: Runtime
    version: str


@dataclass
class FunctionId5(SerializableDataClass):
    inline_context: InlineContext
    code: str
    """
    The inline code to execute
    """
    name: Optional[str] = None
    """
    The name of the inline code function
    """


@dataclass
class FunctionId6(SerializableDataClass):
    inline_prompt: PromptData
    name: Optional[str] = None
    """
    The name of the inline prompt
    """


FunctionId = Union[FunctionId1, FunctionId2, FunctionId3, FunctionId4, FunctionId5, FunctionId6]


@dataclass
class Data4(SerializableDataClass):
    dataset_id: str


@dataclass
class Data5(SerializableDataClass):
    project_name: str
    dataset_name: str


class Collect(Enum):
    ALL = "all"
    NONE = "none"
    SOME = "some"


class Field(Enum):
    COMMIT = "commit"
    BRANCH = "branch"
    TAG = "tag"
    DIRTY = "dirty"
    AUTHOR_NAME = "author_name"
    AUTHOR_EMAIL = "author_email"
    COMMIT_MESSAGE = "commit_message"
    COMMIT_TIME = "commit_time"
    GIT_DIFF = "git_diff"


@dataclass
class GitMetadataSettings(SerializableDataClass):
    collect: Collect
    fields: Optional[List[Field]] = None


@dataclass
class RunEval(SerializableDataClass):
    project_id: str
    """
    Unique identifier for the project to run the eval in
    """
    data: Union[Data4, Data5]
    """
    The dataset to use
    """
    task: FunctionId
    scores: List[FunctionId]
    """
    The functions to score the eval on
    """
    experiment_name: Optional[str] = None
    """
    An optional name for the experiment created by this eval. If it conflicts with an existing experiment, it will be suffixed with a unique identifier.
    """
    metadata: Optional[Dict[str, Any]] = None
    """
    Optional experiment-level metadata to store about the evaluation. You can later use this to slice & dice across experiments.
    """
    stream: Optional[bool] = None
    """
    Whether to stream the results of the eval. If true, the request will return two events: one to indicate the experiment has started, and another upon completion. If false, the request will return the evaluation's summary upon completion.
    """
    trial_count: Optional[float] = None
    """
    The number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.
    """
    is_public: Optional[bool] = None
    """
    Whether the experiment should be public. Defaults to false.
    """
    timeout: Optional[float] = None
    """
    The maximum duration, in milliseconds, to run the evaluation. Defaults to undefined, in which case there is no timeout.
    """
    max_concurrency: Optional[float] = None
    """
    The maximum number of tasks/scorers that will be run concurrently. Defaults to undefined, in which case there is no max concurrency.
    """
    base_experiment_name: Optional[str] = None
    """
    An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.
    """
    base_experiment_id: Optional[str] = None
    """
    An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this experiment.
    """
    git_metadata_settings: Optional[GitMetadataSettings] = None
    """
    Optional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.
    """
    repo_info: Optional[RepoInfo] = None
